Da http://www.themarketingfreaks.com/2017/01/cose-la-sentiment-analysis-utilita-limiti-e-tools-gratis-e-a-pagamento/
PREMESSA
Questa dissertazione ha l’obiettivo di esaminare il campo di studi dell’apprendimento profondo e di illustrarne l’applicazione alle analisi testuali. Nel primo capitolo è introdotta l’analisi del sentimento con le principali metodologie di approccio a questa categoria di problemi. Nei capitoli successivi sono discussi in ordine gerarchico gli ambiti di studio da cui è nato l’apprendimento profondo. Nel secondo si tratteggiano i caratteri fondamentali dell’intelligenza artificiale. All’interno di essa, troviamo il campo dell’apprendimento automatico descritto nel capitolo terzo. Si procede alla descrizione dell’apprendimento profondo e delle reti neurali rispettivamente nei capitoli quattro e cinque. Un esempio pratico di applicazione dell’apprendimento profondo è riportato nel capitolo sei. Infine, nel settimo capitolo, si esplorano le applicazioni di questa analisi in vari contesti.

1 L’ANALISI DEL SENTIMENTO
Citando George R.R. Martin “un lettore vive mille di vite prima di morire, un uomo che non legge mai, vive una vita sola”. A tanto può arrivare la lettura, dal farci immedesimare tra le piccole strade dei quartieri popolari di Parigi tratteggiate da Zola, al sentire la polvere di rena rossa cara a Verga.
Non tutti i testi che leggiamo, però, riescono a suscitare un così forte impatto emotivo. Innumerevoli periodi non hanno alcun tratto artistico ma sono strumentali al passaggio di informazioni. Numerose sono le mansioni che richiedono di allocare molto tempo alla lettura. Questo lavoro risulta spesso tedioso e ripetitivo. Per porre rimedio a questa condizione, nell’ultimo ventennio, molte risorse sono state impiegate per cercare di automatizzare alcune operazioni che riguardano la comprensione o la manipolazione di un corpo di testo. Lo spettro di casistiche interessate da questa ricerca è molto ampio. Dal parafrasare e riassumere, al produrre e correggere, fino ad arrivare all’analisi del significato di un documento, numerose sono state le innovazioni, alimentate dai finanziamenti di enti pubblici ma soprattutto di imprese private, bisognose di strumenti per gestire la crescente interazione con i clienti. 
L’analisi del sentimento è uno di questi strumenti che, grazie all’analisi del linguaggio di un corpo di testo, fornisce la polarità dello stesso. Il sentimento è l’opinione generale sul soggetto, della frase o del testo, che si può dedurre dalla struttura grammaticale o logica dello scritto. Che cosa implica la distinzione tra lettura sintattica o logica di una frase verrà affrontato in seguito. Generalmente la polarità della frase è classificata come negativa, neutra o positiva. “questo gelato è buonissimo” ha una polarità positiva, mentre “il film non è bello ma neanche brutto” ha una polarità neutra. 
Vi sono due grandi famiglie di metodi per svolgere tali analisi: il lexicon based approach  e l’utilizzo dei metodi statistici. Il lexicon based approach consiste nel suddividere l’intero corpo del testo in parti più piccole, dette token, siano essi segmenti di frasi o parole. Successivamente, si effettua la sommatoria del numero di volte in cui una parola ricorre e si assegna un valore di polarità numerico alla stessa. Aggettivi come “bellissimo” o “fantastico” avranno un valore alto sulla scala prescelta, mentre le parole neutre si attestano su valori mediani e parole negative su valori bassi. Per elaborare i token, vi sono due strade percorribili: rilevamento delle keyword e affinità lessicale.
Il rilevamento delle keyword, utilizzato esclusivamente in casistiche semplici, consente di classificare il testo grazie all’individuazione di parole emotivamente significative non ambigue come, per esempio, felice e triste. Queste hanno, rispettivamente un valore di polarità 
alto e basso. Il rilevamento delle keyword non ha requisiti particolari in termini di risorse ma ha il limite di dover avvalersi di un dizionario precostituito.
Il metodo di affinità lessicale parte dai risultati sovra descritti ma aggiunge, in modo arbitrario, un’affinità probabile tra parole con polarità simile. In questo modo si cerca di affinare la selezione delle keyword anche in contesti più complessi. 
L’utilizzo di metodi statistici è la strada più complessa ma più accurata; spesso preferibile. Essi si concretizzano nell’utilizzo di metodologie di apprendimento profondo. Questa opzione ha un grande fabbisogno di risorse, sia in termini computazionali sia in termine di dati necessari per la corretta costituzione del modello classificatore ma presenta un tasso di successo maggiore. La precisione superiore è dovuta all’abbandono dell’utilizzo di un dizionario sostituendolo con la rappresentazione astratta delle frasi stesse prodotta dall’algoritmo. Questo processo riesce a superare alcuni ostacoli tipici delle analisi sui corpi di testo come il sarcasmo, frasi lunghe o rimandi a parti del testo separate tra loro. Per esempio, i metodi tradizionali, raggrupperebbero la frase “la macchina mi ha lasciato a piedi, fantastico!” tra quelle positive, mentre un algoritmo propriamente allenato riesce a individuare casi limite come il sarcasmo o l’ironia.
Per compiere questo tipo di analisi si utilizza una rete neurale ricorrente, una struttura tra le più utilizzate nell’ambito dell’apprendimento profondo. Di seguito sono esposti gli ambiti di studio che hanno permesso di concepire soluzioni per l’analisi del sentimento simili a quelle dell’esempio riportato nel capitolo 6 .



2 INTELLIGENZA ARTIFICIALE
Da https://en.wikipedia.org/wiki/Artificial_intelligence
Intelligenza artificiale è un termine ampio, che racchiude al suo interno molte implicazioni. Quella più importante, risiede nel discernere il vero significato del nome dato a questo campo di studio.  

Nel diciassettesimo secolo Leibniz, Thomas Hobbes e Renè Descartes avevano ipotizzato che i pensieri umani fossero riproducibili con la stessa chiarezza con cui vengono rappresentate forme geometriche o formule matematiche. Questo è stato il primo sforzo, anche se solo concettuale, verso la realizzazione di un’idea verso la quale l’uomo è stato sempre attratto: attribuire caratteristiche antropomorfiche a oggetti che non dovrebbero, per natura, essere animati o godere di coscienza. Si pensi al Golem presente nella tradizione ebraica o al più recente Frankestein di Mary Shelley.
All’inizio degli anni cinquanta del secolo scorso, alcuni tra scienziati e matematici hanno ipotizzato che i recenti avanzamenti della scienza avrebbero permesso di rappresentare il cervello così dettagliatamente che si sarebbe potuta creare una copia informatica che agisse specularmente al suo corrispettivo biologico. Questa è l’idea di intelligenza artificiale generale, un algoritmo che copi perfettamente il pensiero e il ragionamento umano.
Al momento dei primi tentativi, ci si è subito resi conto che per arrivare ad una tale scopo bisognasse, inizialmente, creare programmi che risolvessero problemi più semplici e in un contesto limitato, come risolvere particolari problemi di classificazione o trovare la soluzione migliore in campi complessi, come nel gioco degli scacchi. Da allora siamo rimasti, e forse sempre rimarremo, in questa fase. Quando si parla di intelligenza artificiale, si intendono metodi risolutivi che svolgono estremamente bene determinati compiti e che idealmente dovrebbero avvicinare alla comprensione dell’intelligenza generale.
Possiamo, genericamente, paragonare i vari algoritmi a comuni elettrodomestici. Qualunque elettrodomestico svolge il suo compito meglio di come potrebbe fare una persona, poiché, la sua struttura è funzionale all’ottimizzazione di un processo specifico. Specularmente gli algoritmi che risolvono problemi in ambiti ristretti sono estremamente efficienti a svolgere un determinato compito, ma nulla più.
Non vi è la certezza, ad oggi, di riuscire a produrre un algoritmo che riesca a mimare totalmente i processi cognitivi umani, anche se oggetto di alcune ricerche.  Inoltre ci sono molti elementi che fanno supporre l’intercorrere di un lasso di tempo estremamente l’ungo prima che ci si avvicini anche solo ad avere la tecnologia necessaria al creare un’intelligenza generale. Essa viene, al più, postulata dalla Futurologia, quella branca delle scienze sociali che studia applicazioni e implicazioni di tecnologie non ancora sviluppate.
All’interno di questo campo di studi si può quindi fare una prima e fondamentale distinzione in base agli obiettivi che si vogliono raggiungere. Troviamo knowledge reasoning, planning, machine learning, natural language processing, computer vision, robotics da un lato, che rappresentano gli ambiti ristretti in cui è possibile l’applicazione di metodi statistici e matematici e intelligenza artificiale generale dall’altro.
Spesso l’avanzamento nella risoluzione di una categoria di problemi aiuta anche alla soluzione di altre categorie di quesiti. Ad esempio, fatti miglioramenti nell’ambito della computer vision, passi avanti vengono compiuti nella robotica, grazie agli input migliori che possono essere forniti alle macchine.
Ci possano essere delle trasversalità che legano strettamente obiettivi diversi, ad esempio, tra l’apprendimento automatico e il natural language processing. In questo caso tecnologie diverse lavorano insieme per il superamento di ostacoli comuni, come avviene nel caso dell’analisi del sentimento.
In seguito verranno esaminati solamente elementi riguardanti una delle categorie componenti l’intelligenza artificiale, l’apprendimento automatico, detto anche machine learning.

da https://en.wikipedia.org/wiki/Machine_learning
3 APPRENDIENTO AUTOMATICO 
L’apprendimento automatico, o più comunemente machine learning è un sottoinsieme dell’intelligenza artificiale nel campo della computer science.
Con il termine apprendimento si indica la capacità, attraverso l’utilizzo di tecniche statistiche, di migliorare progressivamente le performance in una data operazione.
L’apprendimento automatico esplora lo studio e la creazione di algoritmi che possono imparare e compiere previsioni su una serie di dati. 
Questi programmi superano una limitazione importante. 
Generalmente, nei problemi di classificazione, si devono prevedere tutte le modalità possibili degli attributi presi in esame. Questa operazione può risultare molto complessa se non impossibile.  Attraverso la creazione di modelli generati sui dati di input, si supera questo ostacolo lasciando al calcolatore l’estrazione di modalità significative. Completata la generazione del modello, l’algoritmo lo userà per compiere l’operazione di classificazione su nuovi dati.
Vi sono due grandi categorie in cui sono divisi i metodi di apprendimento in funzione della presenza o meno di feedback che coadiuvano l’algoritmo. La prima macro-area è chiamata supervised learning poiché gli esempi di input forniti al calcolatore sono corredati dall’output atteso. Vi possono essere casi speciali. In ambianti dinamici, come quello delle vetture a guida autonoma, i dati di input consistono in premi o punizioni in base alla performance dell’algoritmo. In questo caso si parla di Reinforcement Learning.
Nell’eventualità che i feedback siano parziali, parliamo di semi-supervised learning, metre nel caso dove i feedback siano parziali e l’algoritmo debba ottimizzare la scelta tra di essi basandosi su una risorsa scarsa si parla di Active learning.
La seconda macro area consiste nell’unsupervised learning, dove nessun feedback è fornito all’algoritmo. Questa metodologia può essere utilizzata per scoprire correlazioni nascoste tra i dati oppure essere implementata per imparare features aggiuntive.
Nei problemi di classificazione, gli input sono divisi in una o più classi e l’algoritmo deve essere in grado di generare un modello capace di assegnare nuovi input ad una o più delle classi fornite. I compiti di classificazione sono generalmente risolti tramite una metodologia di supervised learning. 
Non solo problemi di classificazione possono essere risolti mediante l’implementazione dell’apprendimento automatico. 
Un'altra categoria similare a quella appena descritta riguarda problemi di regressione. La differenza consiste nell’output che è continuo e non discreto.
Nelle operazioni di clustering, gli input devono essere divisi in gruppi. Essendo questi gruppi originariamente sconosciuti, questa è tipicamente un’operazione di unsupervised learning.
All’interno dell’ambito dell’apprendimento automatico, negli ultimi anni, hanno ottenuto molto successo algoritmi di apprendimento profondo.

4 L’APPRENDIMENTO PROFONDO 
I modelli di apprendimento profondo, come si vedrà in seguito, sono stati ispirati dai patterns di funzionamento del cervello. Nonostante questa ispirazione nella struttura, è bene ricordare che nulla si è ancora avvicinato a simulare la complessità di questo organo.

DA https://it.wikipedia.org/wiki/Apprendimento_profondo
https://en.wikipedia.org/wiki/Deep_learning
Questi algoritmi presentano la caratteristica peculiare di usare una sequenza di layer per l’estrazione delle modalità. Durante questa operazione l’algoritmo genera livelli multipli di rappresentazioni che corrispondono a diversi livelli di astrazione.
Queste architetture vengono usate con o senza feedback.
Sintetizzare in un elenco le principali applicazioni dell’apprendimento profondo sarebbe dispersivo, e si prenderà in esame solo il modello della rete neurale, il più utilizzato e fulcro dell’algoritmo di sentiment analysis, la rete neurale.

5 LE RETI NEURALI
Da https://it.wikipedia.org/wiki/Cogito_ergo_sum
Cogito ergo sum. Con queste tre parole il filosofo e matematico francese ha  riassunto la certezza indubitabile che l’uomo ha di se stesso in quanto individuo pensante. La nostra specie ha fatto del pensiero, del ragionamento, il suo vantaggio competitivo, lungo tutta la traiettoria della sua esistenza. Dall’affrontare la lotta per la sopravvivenza alle mansioni più triviali, l’intelletto è in grado di adattare alcune proprietà fondamentali ad una serie smisurata di compiti.
https://it.wikipedia.org/wiki/Pensiero
l’utilizzo dell’astrazione, passando per la gestione di modelli e simboli attraverso l’iterazione e la ricorsione, fino ad arrivare al dialogo con altre  menti pensanti sono stati gli strumenti per modellare la nostra realtà. 
L’uomo ha sempre creato oggetti in grado di superare limiti fisici e cognitivi. Si pensino si sistemi di leve o carrucole o all’invenzione della scrittura, utile a tramandare una mole di conoscenza impensabile da gestire oralmente. Percorrendo le tappe della storia si è giunti, tra gli anni quaranta e l’inizio degli anni cinquanta del secolo scorso, a gettare le fondamenta per la creazione per eccellenza: riprodurre l’oggetto stesso che produce innovazione. Tra i vari approcci tentati, abbandonati e riscoperti negli ultimi anni, quasi dimenticandosi della loro pluridecennale esistenza, troviamo le reti neurali artificiali.
Questa metodologia consiste nel mimare, con una serie di limitazioni, il cervello umano. 
Le reti neurali artificiali presentano due elementi in comune con le reti neurali biologiche: i neuroni e gli assoni, che generano i diversi percorsi possibili per il passaggio dell’informazione. Una rete neurale artificiale presenta due componenti fondamentali. La prima struttura che si incontra è quella dei layers. In ognuno di essi sono presenti un dato numero di neuroni che portano un’informazione al loro interno. Ogni livello compie una o più operazioni sulle informazioni contenute nei neuroni. Sono sempre presenti negli algoritmi un layer di input, che ha il compito di fornire i dati al layer successivo e il layer di output, avente lo scopo di contenere i risultati desiderati. Ogni neurone e ogni collegamento sono corredati di una serie di parametri, pesi e variabili strumentali a varie operazioni. Alcuni layer vengono definiti hidden layer poiché in essi avvengono le operazioni di ottimizzazione dei parametri gestite generalmente da un algoritmo di backpropagation. Tali operazioni, dovendo gestire matrici multidimensionali, vengono quindi delegate completamente al calcolatore, da qui l’aggettivo hidden. Il flusso dei dati tra i layer è il medesimo per tutte le reti: un layer prende come input l’output del layer precedente e fornisce l’input al layer successivo.
La seconda struttura è data dall’interazione tre i neuroni di un layer e i neuroni del layer successivo. Ogni neurone può essere, o meno, collegato con tutti i neuroni del layer seguente. I collegamenti, oltre a definire la struttura, hanno un grado di attivazione che determina se e quanto l’informazione presente in un neurone debba fluire verso il neurone successivo. Parte del motivo dell’abbandono di questa struttura, nel periodo chiamato inverno dell’intelligenza artificiale, consiste nella sua scarsa capacità di gestire dati sequenziali di cospicua dimensione. Per superare tale ostacolo si utilizza un tipo particolare di rete neurale, la rete neurale ricorrente.
Da https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82
Le reti neurali ricorrenti, o recurrent neural network, sono state create  nella prima metà degli anni ottanta del secolo scorso e hanno iniziato ad ottenere una grande notorietà nell’ultimo decennio grazie al miglioramento nel loro design e alle prestazioni migliori dei processori. Sono indicate nell’elaborazione di dati sequenziali essendo in grado di mantenere all’interno di ogni neurone informazioni relative all’input del neurone precedente. Dati una serie di neuroni, il secondo manterrà le informazioni rilevanti del primo, il terzo manterrà le informazioni rilevanti del secondo e così via. Teoricamente, l’ultimo neurone dovrebbe memorizzare tutto l’array di informazioni, ad esempio una frase, dall’ultima parola alla prima, ma non è sempre così. Con l’aumentare degli input forniti alla rete, ad esempio all’aumentare delle parole di una frase, la rete tenderà a giudicare più significativi gli ultimi input forniti. Per ovviare a questo problema si utilizzano delle unità LSTM (long short term memory) all’interno del network per evitare la perdita di informazioni. Queste unità di memoria permettono alla rete di essere più accurata e sono una delle cause della notorietà di questa tecnologia.

6 UN CASO PRATICO 
https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets
Di seguito viene illustrato l’uso di una rete neurale ricorrente per classificare il sentimento di un campione di recensioni cinematografiche. Questo metodo di apprendimento ricade nella categoria del supervised learning. Gli esempi usati per allenare l’algoritmo sono composti da due informazioni. La prima è la recensione, la seconda è il sentimento effettivo della recensione dedotto da un umano, quindi con accuratezza tendente al cento per cento, che fungerà da feedback nel processo di apprendimento. Il campione è composto di un totale di 22500 recensioni. Ogni recensione è formata da una o più frasi. Il totale delle recensioni è diviso in due set di dati: il primo, chiamato training dataset, viene utilizzato per far apprendere l’algoritmo. Su questi dati avverrà l’ottimizzazione di persi e variabili della rete neurale. Il secondo, chiamato testing dataset, è utilizzato per testare l’efficacia dell’algoritmo.
Essendo derivanti dallo stesso gruppo di campioni, i due dataset avranno un’identica distribuzione di probabilità; ciò rende possibile verificare l’accuratezza dell’algoritmo e controllare la presenza di overfitting: se l’algoritmo ha un’accuratezza maggiore sul training dataset rispetto al testing dataset, implica che ha creato relazioni non esistenti nella totalità dei dati ma soltanto nel sottoinsieme utilizzato per la creazione del modello.
Una volta divisi i dati si procederà alla fase di data preprocessing, durante la quale, si manipoleranno gli input testuali forniti trasformandoli in formati numerici utilizzabili dall’algoritmo. Nell’esempio sono state fatte due operazioni. La prima è stata convertire le informazioni sul sentimento delle recensioni, ovvero il feedback, in due classi numeriche, zero equivalente a un giudizio negativo e uno equivalente a un giudizio positivo. La seconda operazione consiste nel rendere ogni recensione di uguale lunghezza, per favorire le operazioni di calcolo successive.
La fase seguente consiste nel scegliere e creare i layers della rete neurale.
Il primo layer prende il nome di input layer e ha come scopo di introdurre ogni parola delle recensioni in un neurone differente. Le recensioni vengono introdotte singolarmente. Questo livello avrà un numero di neuroni pari al numero di parole della recensione più lunga, da qui l’esigenza di completare i testi più corti con caratteri neutri nella fase di data preprocessing.
Da https://it.wikipedia.org/wiki/Word_embedding
Il secondo layer prende il nome di Embedding layer. Il word embedding permette di creare uno spazio vettoriale continuo che rappresenta la vicinanza sintattica e semantica delle parole all’interno delle frasi. 
Il terzo layer consiste nel Recurrent neural network, del tipo Long Short Term Memory, che ci permette di memorizzare informazioni senza perdite dovute alla lunghezza delle frasi. L’utilizzo della rete LSTM aumenterà l’accuratezza dei risultati. A questo layer si esegue una operazione definita dropout. Il dropout aiuta a ridurre l’overfitting attraverso lo “spegnimento” casuale del collegamento tra alcuni neuroni della rete.
Il quarto layer è denominato fully connected perché presenta la proprietà di avere ogni suo neurone collegato ad ogni singlolo neurone del layer precedente. Dai layer precedenti provengono una serie di learned feature vectors e aggungere un livello fully connected è un modo computazionalmente economico di generare combinazioni non lineari tra essi. In questo livello, una funzione di attivazione trasformerà i valori numerici su scala reale in valori di probabilità in una scala da 0 a 1. Questi valori sono comparabili alle due classi scelte inizialmente per classificare le recensioni. Zero o valori tendenti a esso indicano un sentimento negativo e uno o valori tendenti a esso indicano un sentimento positivo. La funzione di attivazione scelta è la sigmoide. I risultati generati dalla funzione di attivazione sono quelli usati per il calcolo dell’errore rispetto all’output atteso, fornendo i valori confluenti nella funzione di costo. Essa indica lo scostamento della classificazione dell’algoritmo rispetto all’output atteso e minimizzando questa funzione si troveranno i pesi e le variabili per avere un risultato più preciso possibile.
La ricerca del minimo della funzione di costo e l’aggiustamento delle variabili e dei pesi della rete neurale avviene nell’ultimo livello definito regression layer. In questo esempio, per trovare il minimo della funzione di costo si è utilizzata la tecnica della discesa del gradiente.
Dopo alcuni cicli di allenamento l’algoritmo presenta un’accuratezza molto alta che si attesta tendente al 96%.


6 APPLICSZIONI DELL’ANALISI DEL SENTIMENTO
Questo tipo di analisi viene applicato, con i dovuti adattamenti tecnici, a vari ambiti. Enti e imprese usano questa tecnologia per rendere più snello il processo di customer service, riuscendo a filtrare automaticamente molte richieste diminuendo il carico di lavoro del personale di assistenza. 
Immagine pubblica e brand reputation sono due aspetti della vita di persone e imprese il cui monitoraggio viene reso molto più facile. Si possono, infatti, discernere le opinioni riguardo una persona analizzando le frasi scritte dagli utenti in rete. Nel settore pubblico di alcuni paesi esteri sono stati fatti passi per migliorare il rapporto con i cittadini attraverso l’implementazione di applicativi che rendano più immediato e veloce i rapporto con il cittadino. Una possibile applicazione potrebbe essere l’automazione parziale della lettura di alcuni tipi di risposte a domande aperte contenute in sondaggi demoscopici o indagini similari. Sarebbe facilitato il compito di categorizzare le risposte e si fornirebbe uno strumento per gestire facilmente una grande mole di dati.

 
 
 




{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SUPER_State_of_the_Art_Gluco_deepLearning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdyn2Cn6DkUPHMm0PVgJXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bassignana/Thesis/blob/master/SUPER_State_of_the_Art_Gluco_deepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF4mUiv7xipg",
        "outputId": "0cde50d8-49bf-4c4d-a567-eab344a36bcf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d2DGGLOtONf",
        "outputId": "ab336f2e-7042-4af0-abd4-74fa846bceb7"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import keras\n",
        "from datetime import timedelta, datetime\n",
        "from sklearn import preprocessing, linear_model, datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, max_error\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "from scipy.stats import uniform as sp_rand\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pandas.plotting import autocorrelation_plot, lag_plot\n",
        "from statsmodels.graphics.gofplots import qqplot\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from math import sqrt\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "NRzpiqXe-VoN",
        "outputId": "d66cbe12-b76c-4a16-e191-c24dee28dd62"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded1 = files.upload()\n",
        "uploaded2 = files.upload()\n",
        "uploaded3 = files.upload()\n",
        "uploaded4 = files.upload()\n",
        "uploaded5 = files.upload()\n",
        "uploaded6 = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c6e5e97b-1129-46f4-9e3c-d4b73778118b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c6e5e97b-1129-46f4-9e3c-d4b73778118b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving df559_1 to df559_1 (2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9cfaea17-0e5b-4821-be91-93a8f76cbe84\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9cfaea17-0e5b-4821-be91-93a8f76cbe84\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving df563_2 to df563_2 (2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2005384d-d691-4c3a-9d8d-205afa8dedce\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2005384d-d691-4c3a-9d8d-205afa8dedce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving df570_3 to df570_3 (2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-21e0db28-43a9-4833-904d-e0ff6d11d19b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-21e0db28-43a9-4833-904d-e0ff6d11d19b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving df575_4 to df575_4 (2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e0d084bd-dfe8-4e46-a42f-7143b62d4589\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e0d084bd-dfe8-4e46-a42f-7143b62d4589\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving df588_5 to df588_5 (2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a18f8b0-53e1-4869-b43e-c9f0919deca2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5a18f8b0-53e1-4869-b43e-c9f0919deca2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving df591_6 to df591_6 (2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxK3Hs_G9kRT",
        "outputId": "63b7eb80-7cef-452d-9436-cda5565c604a"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ggM-BX-oxW"
      },
      "source": [
        "# To store dataset in a Pandas Dataframe\n",
        "data1 = pd.read_csv(io.BytesIO(uploaded1['df559_1']))\n",
        "data2 = pd.read_csv(io.BytesIO(uploaded2['df563_2']))\n",
        "data3 = pd.read_csv(io.BytesIO(uploaded3['df570_3']))\n",
        "data4 = pd.read_csv(io.BytesIO(uploaded4['df575_4']))\n",
        "data5 = pd.read_csv(io.BytesIO(uploaded5['df588_5']))\n",
        "data6 = pd.read_csv(io.BytesIO(uploaded6['df591_6']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3hg3hsR_FuR"
      },
      "source": [
        "def resample(data, freq):\n",
        "    \"\"\"\n",
        "    :param data: dataframe\n",
        "    :param freq: sampling frequency\n",
        "    :return: resampled data between the the first day at 00:00:00 and the last day at 23:60-freq:00 at freq sample frequency\n",
        "    \"\"\"\n",
        "    start = data.datetime.iloc[0].strftime('%Y-%m-%d') + \" 00:00:00\"\n",
        "    end = datetime.strptime(data.datetime.iloc[-1].strftime('%Y-%m-%d'), \"%Y-%m-%d\") + timedelta(days=1) - timedelta(\n",
        "        minutes=freq)\n",
        "    index = pd.period_range(start=start,\n",
        "                            end=end,\n",
        "                            freq=str(freq) + 'min').to_timestamp()\n",
        "    data = data.resample(str(freq) + 'min', on=\"datetime\").agg({'glucose': np.mean, 'CHO': np.sum, \"insulin\": np.sum})\n",
        "    data = data.reindex(index=index)\n",
        "    data = data.reset_index()\n",
        "    data = data.rename(columns={\"index\": \"datetime\"})\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xONBxEBi_KJr"
      },
      "source": [
        "def create_samples(data, ph, hist):\n",
        "    n_samples = data.shape[0] - ph - hist + 1\n",
        "    # number of rows\n",
        "    y = data.loc[ph + hist - 1:, \"glucose\"].values.reshape(-1, 1)\n",
        "    d = pd.DatetimeIndex(data.loc[ph + hist - 1:, \"datetime\"].values)\n",
        "       # t = np.concatenate([np.arange(day_len) for _ in range(len(data) // day_len)], axis=0)[ph + hist - 1:].reshape(-1, 1)\n",
        "    g = np.array([data.loc[i:i + n_samples - 1, \"glucose\"] for i in range(hist)]).transpose()\n",
        "    c = np.array([data.loc[i:i + n_samples - 1, \"CHO\"] for i in range(hist)]).transpose()\n",
        "    i = np.array([data.loc[i:i + n_samples - 1, \"insulin\"] for i in range(hist)]).transpose()\n",
        "    \n",
        "    new_columns = np.r_[[\"glucose_\" + str(i) for i in range(hist)], [\"CHO_\" + str(i) for i in range(hist)], [\n",
        "            \"insulin_\" + str(i) for i in range(hist)], [\"y\"]]\n",
        "    len(new_columns)\n",
        "    new_data = pd.DataFrame(data=np.c_[g, c, i, y], columns=new_columns)\n",
        "    new_data[\"datetime\"] = d\n",
        "    new_data = new_data.loc[:, np.r_[[\"datetime\"], new_columns]]  # reorder the columns, with datetime first\n",
        "    return new_data\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPZns_odwGzT"
      },
      "source": [
        "\"\"\"\n",
        "Implementation of Clarke and Parkes error grid analysis\n",
        "\"\"\"\n",
        "def clarke_error_zone_detailed(act, pred):\n",
        "    \"\"\"\n",
        "    This function outputs the Clarke Error Grid region (encoded as integer)\n",
        "    for a combination of actual and predicted value\n",
        "    Based on 'Evaluating clinical accuracy of systems for self-monitoring of blood glucose':\n",
        "    https://care.diabetesjournals.org/content/10/5/622\n",
        "    \"\"\"\n",
        "    # Zone A\n",
        "    if (act < 70 and pred < 70) or abs(act - pred) < 0.2 * act:\n",
        "        return 0\n",
        "    # Zone E - left upper\n",
        "    if act <= 70 and pred >= 180:\n",
        "        return 8\n",
        "    # Zone E - right lower\n",
        "    if act >= 180 and pred <= 70:\n",
        "        return 7\n",
        "    # Zone D - right\n",
        "    if act >= 240 and 70 <= pred <= 180:\n",
        "        return 6\n",
        "    # Zone D - left\n",
        "    if act <= 70 <= pred <= 180:\n",
        "        return 5\n",
        "    # Zone C - upper\n",
        "    if 70 <= act <= 290 and pred >= act + 110:\n",
        "        return 4\n",
        "    # Zone C - lower\n",
        "    if 130 <= act <= 180 and pred <= (7/5) * act - 182:\n",
        "        return 3\n",
        "    # Zone B - upper\n",
        "    if act < pred:\n",
        "        return 2\n",
        "    # Zone B - lower\n",
        "    return 1\n",
        "\n",
        "def parkes_error_zone_detailed(act, pred, diabetes_type):\n",
        "    \"\"\"\n",
        "    This function outputs the Parkes Error Grid region (encoded as integer)\n",
        "    for a combination of actual and predicted value\n",
        "    for type 1 and type 2 diabetic patients\n",
        "    Based on the article 'Technical Aspects of the Parkes Error Grid':\n",
        "    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3876371/\n",
        "    \"\"\"\n",
        "    def above_line(x_1, y_1, x_2, y_2, strict=False):\n",
        "        if x_1 == x_2:\n",
        "            return False\n",
        "\n",
        "        y_line = ((y_1 - y_2) * act + y_2 * x_1 - y_1 * x_2) / (x_1 - x_2)\n",
        "        return pred > y_line if strict else pred >= y_line\n",
        "\n",
        "    def below_line(x_1, y_1, x_2, y_2, strict=False):\n",
        "        return not above_line(x_1, y_1, x_2, y_2, not strict)\n",
        "\n",
        "    def parkes_type_1(act, pred):\n",
        "        # Zone E\n",
        "        if above_line(0, 150, 35, 155) and above_line(35, 155, 50, 550):\n",
        "            return 7\n",
        "        # Zone D - left upper\n",
        "        if (pred > 100 and above_line(25, 100, 50, 125) and\n",
        "                above_line(50, 125, 80, 215) and above_line(80, 215, 125, 550)):\n",
        "            return 6\n",
        "        # Zone D - right lower\n",
        "        if (act > 250 and below_line(250, 40, 550, 150)):\n",
        "            return 5\n",
        "        # Zone C - left upper\n",
        "        if (pred > 60 and above_line(30, 60, 50, 80) and\n",
        "                above_line(50, 80, 70, 110) and above_line(70, 110, 260, 550)):\n",
        "            return 4\n",
        "        # Zone C - right lower\n",
        "        if (act > 120 and below_line(120, 30, 260, 130) and below_line(260, 130, 550, 250)):\n",
        "            return 3\n",
        "        # Zone B - left upper\n",
        "        if (pred > 50 and above_line(30, 50, 140, 170) and\n",
        "                above_line(140, 170, 280, 380) and (act < 280 or above_line(280, 380, 430, 550))):\n",
        "            return 2\n",
        "        # Zone B - right lower\n",
        "        if (act > 50 and below_line(50, 30, 170, 145) and\n",
        "                below_line(170, 145, 385, 300) and (act < 385 or below_line(385, 300, 550, 450))):\n",
        "            return 1\n",
        "        # Zone A\n",
        "        return 0\n",
        "\n",
        "    def parkes_type_2(act, pred):\n",
        "        # Zone E\n",
        "        if (pred > 200 and above_line(35, 200, 50, 550)):\n",
        "            return 7\n",
        "        # Zone D - left upper\n",
        "        if (pred > 80 and above_line(25, 80, 35, 90) and above_line(35, 90, 125, 550)):\n",
        "            return 6\n",
        "        # Zone D - right lower\n",
        "        if (act > 250 and below_line(250, 40, 410, 110) and below_line(410, 110, 550, 160)):\n",
        "            return 5\n",
        "        # Zone C - left upper\n",
        "        if (pred > 60 and above_line(30, 60, 280, 550)):\n",
        "            return 4\n",
        "        # Zone C - right lower\n",
        "        if (below_line(90, 0, 260, 130) and below_line(260, 130, 550, 250)):\n",
        "            return 3\n",
        "        # Zone B - left upper\n",
        "        if (pred > 50 and above_line(30, 50, 230, 330) and\n",
        "                (act < 230 or above_line(230, 330, 440, 550))):\n",
        "            return 2\n",
        "        # Zone B - right lower\n",
        "        if (act > 50 and below_line(50, 30, 90, 80) and below_line(90, 80, 330, 230) and\n",
        "                (act < 330 or below_line(330, 230, 550, 450))):\n",
        "            return 1\n",
        "        # Zone A\n",
        "        return 0\n",
        "\n",
        "    if diabetes_type == 1:\n",
        "        return parkes_type_1(act, pred)\n",
        "\n",
        "    if diabetes_type == 2:\n",
        "        return parkes_type_2(act, pred)\n",
        "\n",
        "    raise Exception('Unsupported diabetes type')\n",
        "\n",
        "clarke_error_zone_detailed = np.vectorize(clarke_error_zone_detailed)\n",
        "parkes_error_zone_detailed = np.vectorize(parkes_error_zone_detailed)\n",
        "\n",
        "def zone_accuracy(act_arr, pred_arr, mode='clarke', detailed=False, diabetes_type=1):\n",
        "    \"\"\"\n",
        "    Calculates the average percentage of each zone based on Clarke or Parkes\n",
        "    Error Grid analysis for an array of predictions and an array of actual values\n",
        "    \"\"\"\n",
        "    acc = np.zeros(9)\n",
        "    if mode == 'clarke':\n",
        "        res = clarke_error_zone_detailed(act_arr, pred_arr)\n",
        "    elif mode == 'parkes':\n",
        "        res = parkes_error_zone_detailed(act_arr, pred_arr, diabetes_type)\n",
        "    else:\n",
        "        raise Exception('Unsupported error grid mode')\n",
        "\n",
        "    acc_bin = np.bincount(res)\n",
        "    acc[:len(acc_bin)] = acc_bin\n",
        "\n",
        "    if not detailed:\n",
        "        acc[1] = acc[1] + acc[2]\n",
        "        acc[2] = acc[3] + acc[4]\n",
        "        acc[3] = acc[5] + acc[6]\n",
        "        acc[4] = acc[7] + acc[8]\n",
        "        acc = acc[:5]\n",
        "\n",
        "    return acc / sum(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JVvh34o_KUw"
      },
      "source": [
        "def extract_training_data(data):\n",
        "        \"\"\"\n",
        "        Extract the input variables (x), the time (t), and the objective (y) from the data samples.\n",
        "        WARNING : need to be modified to include additional data, or override the function within the models\n",
        "        :param data:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \n",
        "        y = data[\"y\"]\n",
        "        x = data.drop([\"y\", \"datetime\"], axis=1)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def extract_training_glucose(data):\n",
        "        \"\"\"\n",
        "        Extract the input variables (x), the time (t), and the objective (y) from the data samples.\n",
        "        WARNING : need to be modified to include additional data, or override the function within the models\n",
        "        :param data:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        y = data[\"y\"]\n",
        "        x = data.filter(like='glucose',axis=1)\n",
        "        return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4APAaX0VInVR"
      },
      "source": [
        "dt = pd.DataFrame()\n",
        "for data in [data1,data2,data3,data4,data5,data6]:\n",
        "  data['datetime'] =  pd.to_datetime(data['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
        "  data_resampled = resample(data, 5)\n",
        "  data_resampled['glucose'].isna().sum()\n",
        "  data_resampled.dropna(inplace = True)\n",
        "  data_resampled['glucose'].isna().sum()\n",
        "#fill na's with zeros\n",
        "  for col in data_resampled.columns:\n",
        "      if \"insulin\" in col or \"CHO\" in col:\n",
        "          data_resampled[col] = data_resampled[col].fillna(0)\n",
        "  dt = pd.concat([data_resampled, dt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "w-Vk1N4eK6E0",
        "outputId": "1e483efe-5f32-413e-b4e2-ea711f860776"
      },
      "source": [
        "dt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>glucose</th>\n",
              "      <th>CHO</th>\n",
              "      <th>insulin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>2021-11-30 17:05:00</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>2021-11-30 17:10:00</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>2021-11-30 17:15:00</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>2021-11-30 17:20:00</td>\n",
              "      <td>166.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>2021-11-30 17:25:00</td>\n",
              "      <td>175.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12091</th>\n",
              "      <td>2022-01-17 23:35:00</td>\n",
              "      <td>161.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12092</th>\n",
              "      <td>2022-01-17 23:40:00</td>\n",
              "      <td>164.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12093</th>\n",
              "      <td>2022-01-17 23:45:00</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12094</th>\n",
              "      <td>2022-01-17 23:50:00</td>\n",
              "      <td>172.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12095</th>\n",
              "      <td>2022-01-17 23:55:00</td>\n",
              "      <td>176.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>69255 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 datetime  glucose  CHO  insulin\n",
              "205   2021-11-30 17:05:00    160.0  0.0      0.0\n",
              "206   2021-11-30 17:10:00    158.0  0.0      0.0\n",
              "207   2021-11-30 17:15:00    160.0  0.0      0.0\n",
              "208   2021-11-30 17:20:00    166.0  0.0      0.0\n",
              "209   2021-11-30 17:25:00    175.0  0.0      0.0\n",
              "...                   ...      ...  ...      ...\n",
              "12091 2022-01-17 23:35:00    161.0  0.0      0.0\n",
              "12092 2022-01-17 23:40:00    164.0  0.0      0.0\n",
              "12093 2022-01-17 23:45:00    168.0  0.0      0.0\n",
              "12094 2022-01-17 23:50:00    172.0  0.0      0.0\n",
              "12095 2022-01-17 23:55:00    176.0  0.0      0.0\n",
              "\n",
              "[69255 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEmXS6deBH9w"
      },
      "source": [
        "#data['datetime'] =  pd.to_datetime(data['datetime'], format='%Y-%m-%d %H:%M:%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "dQOJFRZ5GwF6",
        "outputId": "6c6feb5a-22bf-4f70-80cb-55d40da15055"
      },
      "source": [
        "#data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>glucose</th>\n",
              "      <th>CHO</th>\n",
              "      <th>insulin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-12-07 01:08:04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-12-07 01:17:00</td>\n",
              "      <td>101.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-12-07 01:22:00</td>\n",
              "      <td>98.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-12-07 01:27:00</td>\n",
              "      <td>104.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-12-07 01:32:00</td>\n",
              "      <td>112.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11280</th>\n",
              "      <td>2022-01-13 23:38:00</td>\n",
              "      <td>266.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11281</th>\n",
              "      <td>2022-01-13 23:43:00</td>\n",
              "      <td>275.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11282</th>\n",
              "      <td>2022-01-13 23:48:00</td>\n",
              "      <td>268.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11283</th>\n",
              "      <td>2022-01-13 23:53:00</td>\n",
              "      <td>301.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11284</th>\n",
              "      <td>2022-01-13 23:58:00</td>\n",
              "      <td>290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71586 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 datetime  glucose  CHO  insulin\n",
              "0     2021-12-07 01:08:04      NaN  NaN      1.6\n",
              "1     2021-12-07 01:17:00    101.0  NaN      NaN\n",
              "2     2021-12-07 01:22:00     98.0  NaN      NaN\n",
              "3     2021-12-07 01:27:00    104.0  NaN      NaN\n",
              "4     2021-12-07 01:32:00    112.0  NaN      NaN\n",
              "...                   ...      ...  ...      ...\n",
              "11280 2022-01-13 23:38:00    266.0  NaN      NaN\n",
              "11281 2022-01-13 23:43:00    275.0  NaN      NaN\n",
              "11282 2022-01-13 23:48:00    268.0  NaN      NaN\n",
              "11283 2022-01-13 23:53:00    301.0  NaN      NaN\n",
              "11284 2022-01-13 23:58:00    290.0  NaN      NaN\n",
              "\n",
              "[71586 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnxUNMCG_Hco"
      },
      "source": [
        "#data_resampled = resample(data, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "_ltxrjD3HRXy",
        "outputId": "892dc509-d2a9-4f39-a334-003494ba274c"
      },
      "source": [
        "#data_resampled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>glucose</th>\n",
              "      <th>CHO</th>\n",
              "      <th>insulin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-12-07 00:00:00</td>\n",
              "      <td>133.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-12-07 00:05:00</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-12-07 00:10:00</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-12-07 00:15:00</td>\n",
              "      <td>126.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-12-07 00:20:00</td>\n",
              "      <td>124.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10939</th>\n",
              "      <td>2022-01-13 23:35:00</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10940</th>\n",
              "      <td>2022-01-13 23:40:00</td>\n",
              "      <td>186.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10941</th>\n",
              "      <td>2022-01-13 23:45:00</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10942</th>\n",
              "      <td>2022-01-13 23:50:00</td>\n",
              "      <td>193.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10943</th>\n",
              "      <td>2022-01-13 23:55:00</td>\n",
              "      <td>187.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10944 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 datetime     glucose  CHO  insulin\n",
              "0     2021-12-07 00:00:00  133.000000  0.0      0.0\n",
              "1     2021-12-07 00:05:00  129.000000  0.0      0.0\n",
              "2     2021-12-07 00:10:00  126.000000  0.0      0.0\n",
              "3     2021-12-07 00:15:00  126.500000  0.0      0.0\n",
              "4     2021-12-07 00:20:00  124.000000  0.0      0.0\n",
              "...                   ...         ...  ...      ...\n",
              "10939 2022-01-13 23:35:00  185.000000  0.0      4.1\n",
              "10940 2022-01-13 23:40:00  186.666667  0.0      0.0\n",
              "10941 2022-01-13 23:45:00  183.000000  0.0      0.0\n",
              "10942 2022-01-13 23:50:00  193.000000  0.0      0.0\n",
              "10943 2022-01-13 23:55:00  187.666667  0.0      0.0\n",
              "\n",
              "[10944 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlQkJqZaCXSe"
      },
      "source": [
        "#data_resampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsSUCBG7eeqL",
        "outputId": "03d3a5b6-0206-492f-f57a-68a32b4ce0eb"
      },
      "source": [
        "#data_resampled['glucose'].isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbF70O_d_JvQ"
      },
      "source": [
        "#data_resampled[\"glucose\"].interpolate(method = \"polynomial\", order = 5, inplace = True, limit = 5)#impostare limit se no vengono dei valori di glucosio negativi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uheu1OKYgST",
        "outputId": "135ddf7a-ddb5-4233-c718-281fa9fbdef7"
      },
      "source": [
        "#data_resampled['glucose'].isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Wa2dW_YpKT",
        "outputId": "f2eaaa67-9389-4e5c-f123-95d993b0d76f"
      },
      "source": [
        "#data_resampled.dropna(inplace = True)\n",
        "#data_resampled['glucose'].isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC6weCN6_J8V"
      },
      "source": [
        "#fill na's with zeros\n",
        "#for col in data_resampled.columns:\n",
        "#    if \"insulin\" in col or \"CHO\" in col:\n",
        "#        data_resampled[col] = data_resampled[col].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtiHkTfl_J-7"
      },
      "source": [
        "data_resampled = dt\n",
        "#train test split\n",
        "n = len(data_resampled)\n",
        "#n\n",
        "train_df = data_resampled[0:int(n*0.9)]\n",
        "test_df = data_resampled[int(n*0.9):int(n*1)]\n",
        "#train_df.shape\n",
        "#test_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go8uqoPdiI6o"
      },
      "source": [
        "#dati per il riscaling\n",
        "risc_min = train_df['glucose'].min()\n",
        "risc_range = train_df['glucose'].max()-train_df['glucose'].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF9vT9Ao_KBd",
        "outputId": "5ad81b64-7bb0-4ac7-b7b6-8530becf7729"
      },
      "source": [
        "#standardizzazione\n",
        "x_cols = [x for x in train_df.columns if x != 'datetime']\n",
        "min_max_scaler = preprocessing.MinMaxScaler()# 0 1 scale\n",
        "train_df = min_max_scaler.fit_transform(train_df[x_cols])\n",
        "#handle sparses data well, but re read documentation\n",
        "#how to apply the same transformation to the tast set\n",
        "#check if correct!\n",
        "test_df = min_max_scaler.transform(test_df[x_cols])\n",
        "#recreate the dataframe\n",
        "train_df = pd.DataFrame(data=train_df, columns=x_cols)\n",
        "test_df = pd.DataFrame(data=test_df, columns=x_cols)\n",
        "\n",
        "train_df.shape\n",
        "test_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6926, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFj4JXlr_KGg"
      },
      "source": [
        "train_df[\"datetime\"] = pd.DatetimeIndex(data_resampled.iloc[:int(len(train_df[\"glucose\"])), 0].values)\n",
        "test_df[\"datetime\"] = pd.DatetimeIndex(data_resampled.iloc[len(train_df[\"glucose\"]):n, 0].values)\n",
        "\n",
        "train_df = train_df[['datetime',\"glucose\", \"CHO\", \"insulin\"]]\n",
        "test_df = test_df[['datetime',\"glucose\", \"CHO\", \"insulin\"]]\n",
        "\n",
        "#train_df.shape\n",
        "#test_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwyH6f9F_KML"
      },
      "source": [
        "train = create_samples(train_df, 6, 24)\n",
        "train.dropna(inplace = True)   \n",
        "\n",
        "test = create_samples(test_df, 6, 24)\n",
        "test.dropna(inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnwlAnVL_KPN"
      },
      "source": [
        "######################################################################\n",
        "#basic univariate lstm models\n",
        "######################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2xfzC7OVBHi"
      },
      "source": [
        "xtrain, ytrain = extract_training_glucose(train)\n",
        "xtest, ytest = extract_training_glucose(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6lZPwy0_KFB"
      },
      "source": [
        "xtrain.values.shape\n",
        "xtrain_lstm = xtrain.values.reshape((xtrain.shape[0],xtrain.shape[1],1))\n",
        "#xtrain_lstm.shape#ok\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbmX2-DQXTWF"
      },
      "source": [
        "xtest_lstm = xtest.values.reshape((xtest.shape[0],xtest.shape[1],1))\n",
        "#xtest_lstm.shape#ok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23hwzKiOVjla"
      },
      "source": [
        "n_steps = xtrain_lstm.shape[1]\n",
        "n_features = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zABeaqL5VjrZ"
      },
      "source": [
        "#vanilla lstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBnRSmpXVjv9"
      },
      "source": [
        "# define model vanilla \n",
        "#model = Sequential()\n",
        "#model.add(LSTM(20, activation='relu', input_shape=(n_steps,\n",
        "#n_features)))\n",
        "#model.add(Dense(1))\n",
        "#model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "#model.fit(xtrain_lstm, ytrain, epochs=100, verbose=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qn7RYoCT15G"
      },
      "source": [
        "# mi servono xtest rescaled e y test rescaled per la CV\n",
        "#xtest_resc = xtest.transform(lambda x: x*risc_range + risc_min )\n",
        "#xtest_resc_lstm = xtest_resc.values.reshape((xtest_resc.shape[0],xtest_resc.shape[1],1))\n",
        "#xtest_resc_lstm.shape#ok\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-JFTOrQVj3o"
      },
      "source": [
        "#yhat = model.predict(xtest_lstm, verbose=0)\n",
        "\n",
        "#yhat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL0uLAWskWR_"
      },
      "source": [
        "#per rifare lo scaling al contrario devo moltiplicare ogni valore di yhat per il resc range ed aggiungervi il minimo\n",
        "#yhat_rescaled = yhat*risc_range + risc_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxbzcqQuk4-E"
      },
      "source": [
        "ytest_rescaled = ytest*risc_range + risc_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16Yv2xxgynvA"
      },
      "source": [
        "clarke_results = pd.DataFrame()\n",
        "err_results = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDbwUscH_GkS"
      },
      "source": [
        "Se l'sltm segnala che non userà cuDNN provare a fare ciò che è srcritto qui\n",
        "\n",
        "The requirements to use the cuDNN implementation are:\n",
        "*   activation == tanh\n",
        "*   recurrent_activation == sigmoid\n",
        "*   recurrent_dropout == 0\n",
        "*   unroll is False\n",
        "*   use_bias is True\n",
        "*   Inputs, if use masking, are strictly right-padded.\n",
        "*   Eager execution is enabled in the outermost context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW98ae3RIO0j"
      },
      "source": [
        "You can use a trained model without having to retrain it, or pick-up training where you left off in case the training process was interrupted. The tf.keras.callbacks.ModelCheckpoint callback allows you to continually save the model both during and at the end of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C4XMIRJUsZn"
      },
      "source": [
        "# Include the epoch in the file name (uses `str.format`)\n",
        "#checkpoint_path = \"training_1/cp-{epoch:04d}.ckpt\"\n",
        "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "\n",
        "#cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#    filepath=checkpoint_path, \n",
        "#    monitor = 'val_loss'\n",
        "#    mode = 'min',\n",
        "#    verbose=1, \n",
        "#    save_weights_only = True,\n",
        "#    save_best_only = True)\n",
        "\n",
        "#es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "#    monitor='val_loss', min_delta=0.0005, patience=0, verbose=0, mode='min',\n",
        "#    baseline=None, restore_best_weights=True\n",
        "#)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yage5iDX5s1x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWBvX5PX5xRS"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='tr_loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  #plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y02G-wXZVkTR"
      },
      "source": [
        "# CNN LSTM\n",
        "# se ho 24 colonne nel livello di glucosio li devo dividere in un tot di sequenze n_seq con ognuna un tot di colonne n_steps\n",
        "# es 2*12\n",
        "\n",
        "n_seq = 2\n",
        "n_steps = 12\n",
        "n_features = 1\n",
        "xtrain_CNN_lstm = xtrain.values.reshape((xtrain.shape[0], n_seq, n_steps, n_features))\n",
        "xtest_CNN_lstm = xtest.values.reshape((xtest.shape[0],n_seq, n_steps, n_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy5EzFLEdmlG",
        "outputId": "b6f7f6fe-f1dd-4094-a6c2-3355c84c7f45"
      },
      "source": [
        "xtrain_CNN_lstm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62300, 2, 12, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYHuT3QJ6Yj_"
      },
      "source": [
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"training_4/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    verbose=0, \n",
        "    save_weights_only = True,\n",
        "    save_best_only = True)\n",
        "\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0.00000001, patience=100000, verbose=0, mode='min',\n",
        "    baseline=None, restore_best_weights=True\n",
        ")\n",
        "\n",
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
        "                                     factor=0.2, \n",
        "                                     patience=10, \n",
        "                                     verbose=1, \n",
        "                                     mode='min',    \n",
        "                                     min_delta=0.001, \n",
        "                                     cooldown=0, \n",
        "                                     min_lr=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2QzZ96zgJmC"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'), input_shape=(None, n_steps, n_features))) \n",
        "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "model.add(LSTM(128, activation='tanh', dropout = 0.3)) \n",
        "model.add(Dense(1))\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='mse') # fit model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "XIkc1wCjqEtM",
        "outputId": "3986a2a2-8d11-4e73-8092-e23f409fd945"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
        "    rankdir='TB', expand_nested=True, dpi=80\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMAs0uDLiIIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9e83d4-f215-44c6-deac-a253f87eb945"
      },
      "source": [
        "history = model.fit(xtrain_CNN_lstm, ytrain, epochs=200, verbose=1,validation_split= 0.2,callbacks=[cp_callback, es_callback, lr_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0066 - val_loss: 0.0045\n",
            "Epoch 2/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0052 - val_loss: 0.0044\n",
            "Epoch 3/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0050 - val_loss: 0.0045\n",
            "Epoch 4/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0049 - val_loss: 0.0055\n",
            "Epoch 5/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0049 - val_loss: 0.0043\n",
            "Epoch 6/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 7/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0047 - val_loss: 0.0042\n",
            "Epoch 8/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 9/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0046 - val_loss: 0.0042\n",
            "Epoch 10/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0046 - val_loss: 0.0042\n",
            "Epoch 11/200\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.0046\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0046 - val_loss: 0.0043\n",
            "Epoch 12/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 13/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 14/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 15/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 16/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0043 - val_loss: 0.0040\n",
            "Epoch 17/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 18/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0040\n",
            "Epoch 19/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 20/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 21/200\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.0042\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 22/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0040\n",
            "Epoch 23/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 24/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0040\n",
            "Epoch 25/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 26/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0040\n",
            "Epoch 27/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 28/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 29/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 30/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 31/200\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 32/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 33/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 34/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 35/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 36/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 37/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 38/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 39/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 40/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 41/200\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 42/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 43/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 44/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 45/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 46/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 47/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 48/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 49/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 50/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 51/200\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 52/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 53/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 54/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 55/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 56/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 57/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 58/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 59/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 60/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 61/200\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 62/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 63/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 64/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 65/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 66/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 67/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 68/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 69/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 70/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 71/200\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 72/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 73/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 74/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 75/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 76/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 77/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 78/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 79/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 80/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 81/200\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 82/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 83/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 84/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 85/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 86/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 87/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 88/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 89/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 90/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 91/200\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 92/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 93/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 94/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 95/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 96/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 97/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 98/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 99/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 100/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 101/200\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 102/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 103/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 104/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 105/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 106/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 107/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 108/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 109/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 110/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 111/200\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 112/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 113/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 114/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 115/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 116/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 117/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 118/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 119/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 120/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 121/200\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 122/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 123/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 124/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 125/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 126/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 127/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 128/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 129/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 130/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 131/200\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 132/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 133/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 134/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 135/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 136/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 137/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 138/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 139/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 140/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 141/200\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1.6384001360475466e-13.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 142/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 143/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 144/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 145/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 146/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 147/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 148/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 149/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 150/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 151/200\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 3.2768002178849846e-14.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 152/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 153/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 154/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 155/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 156/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 157/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 158/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 159/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 160/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 161/200\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 6.553600300244697e-15.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 162/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 163/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 164/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 165/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 166/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 167/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 168/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 169/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 170/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 171/200\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 1.3107200431082805e-15.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 172/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 173/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 174/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 175/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 176/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 177/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 178/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 179/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 180/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 181/200\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 2.6214401285682084e-16.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 182/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 183/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 184/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 185/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 186/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 187/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 188/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 189/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 190/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 191/200\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 5.2428803630155353e-17.\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 192/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 193/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 194/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 195/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 196/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 197/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 198/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 199/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 200/200\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.0041 - val_loss: 0.0040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX1hRCYvSuof",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8aa59256-7c80-4374-b39e-65991b2e7709"
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>lr</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.005224</td>\n",
              "      <td>5.242880e-17</td>\n",
              "      <td>195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>0.002376</td>\n",
              "      <td>0.005224</td>\n",
              "      <td>5.242880e-17</td>\n",
              "      <td>196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>0.002383</td>\n",
              "      <td>0.005224</td>\n",
              "      <td>5.242880e-17</td>\n",
              "      <td>197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>0.002378</td>\n",
              "      <td>0.005224</td>\n",
              "      <td>5.242880e-17</td>\n",
              "      <td>198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>0.002372</td>\n",
              "      <td>0.005224</td>\n",
              "      <td>5.242880e-17</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss  val_loss            lr  epoch\n",
              "195  0.002348  0.005224  5.242880e-17    195\n",
              "196  0.002376  0.005224  5.242880e-17    196\n",
              "197  0.002383  0.005224  5.242880e-17    197\n",
              "198  0.002378  0.005224  5.242880e-17    198\n",
              "199  0.002372  0.005224  5.242880e-17    199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_DUUOs5Sx_z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "bc2ee183-7d6e-458e-9a99-40302ce77287"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivDtz6o9i07S",
        "outputId": "1dbd42b2-8985-45a2-bff8-22855664bfaa"
      },
      "source": [
        "yhat = model.predict(xtest_CNN_lstm, verbose=0)\n",
        "yhat_rescaled = yhat*risc_range + risc_min\n",
        "rmse = sqrt(mean_squared_error(ytest_rescaled, yhat_rescaled)) \n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 27.642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdNesOJF3ZAD"
      },
      "source": [
        "yy = yhat_rescaled.reshape(yhat_rescaled.shape[0])\n",
        "res = zone_accuracy(ytest_rescaled, yy, mode='clarke', detailed=False, diabetes_type=1)\n",
        "clarke_results = clarke_results.append(pd.DataFrame(res), ignore_index=False, verify_integrity=False, sort=None)\n",
        "a = max_error(ytest_rescaled, yy)\n",
        "b = sqrt(mean_squared_error(ytest_rescaled, yhat_rescaled))\n",
        "c = mean_absolute_error(ytest_rescaled, yy)\n",
        "resarr = [a,b,c]\n",
        "err_results = err_results.append(pd.DataFrame(resarr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR8HO8nE6Flx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "f18f382a-f860-46ce-c653-947d58cb99cb"
      },
      "source": [
        "err_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>51.047150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.958291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.548484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0\n",
              "0  51.047150\n",
              "1  13.958291\n",
              "2  10.548484"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYe2R8RKlbPC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0e1d82b5-a55e-4efa-9900-d1ff5c1c3109"
      },
      "source": [
        "clarke_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.976548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.023452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0\n",
              "0  0.976548\n",
              "1  0.023452\n",
              "2  0.000000\n",
              "3  0.000000\n",
              "4  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSYVg9UYqyu-"
      },
      "source": [
        "tot_iter = 10\n",
        "\n",
        "es_monitor = ['val_loss']*tot_iter\n",
        "es_min_delta = [0.00000001]*tot_iter\n",
        "es_patience = [100000]*tot_iter\n",
        "es_mode = ['min']*tot_iter\n",
        "ls_monitor=['val_loss']*tot_iter\n",
        "ls_factor=list(np.linspace(start = 0.001, stop = 0.999, num = tot_iter)) \n",
        "ls_patience=[10]*tot_iter \n",
        "ls_verbose=[1]*tot_iter \n",
        "ls_mode=['min']*tot_iter    \n",
        "ls_min_delta=[0.001]*tot_iter\n",
        "ls_cooldown=[0]*tot_iter\n",
        "ls_min_lr=[0]*tot_iter\n",
        "filters=[128]*tot_iter\n",
        "kernel_size=[3]*tot_iter\n",
        "activation=['relu']*tot_iter\n",
        "pool_size=[2]*tot_iter\n",
        "lstm_units=[128]*tot_iter\n",
        "lstm_dropout=[0.3]*tot_iter\n",
        "learning_rate=[0.001]*tot_iter\n",
        "epochs=[200]*tot_iter\n",
        "\n",
        "#essendo zip un iterabile, dopo il primo giro sparisce tutto se non assegno list() a qualcosa\n",
        "par = list(zip(es_monitor ,\n",
        "es_min_delta ,\n",
        "es_patience ,\n",
        "es_mode ,#3\n",
        "ls_monitor,\n",
        "ls_factor, \n",
        "ls_patience, \n",
        "ls_verbose, \n",
        "ls_mode,    \n",
        "ls_min_delta,\n",
        "ls_cooldown,\n",
        "ls_min_lr,#11\n",
        "filters,\n",
        "kernel_size,\n",
        "activation,\n",
        "pool_size,#15\n",
        "lstm_units,\n",
        "lstm_dropout,\n",
        "learning_rate,\n",
        "epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfcpmXLMfbPk",
        "outputId": "c17eab8a-fc48-4a5b-8165-fbca21e7be3a"
      },
      "source": [
        "#ricordarsi che nel range non genera anche la cifra d'arrivo\n",
        "for i in range(0,tot_iter):\n",
        "  # Include the epoch in the file name (uses `str.format`)\n",
        "  checkpoint_path = \"training_4/cp-{epoch:04d}.ckpt\"\n",
        "  checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    verbose=0, \n",
        "    save_weights_only = True,\n",
        "    save_best_only = True)\n",
        "\n",
        "  es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=par[i][0], min_delta=par[i][1], patience=par[i][2], verbose=0, mode=par[i][3],\n",
        "    baseline=None, restore_best_weights=True\n",
        "  )\n",
        "\n",
        "  lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=par[i][4], \n",
        "                                     factor=par[i][5], \n",
        "                                     patience=par[i][6], \n",
        "                                     verbose=par[i][7], \n",
        "                                     mode=par[i][8],    \n",
        "                                     min_delta=par[i][9], \n",
        "                                     cooldown=par[i][10], \n",
        "                                     min_lr=par[i][11])\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(TimeDistributed(Conv1D(filters=par[i][12], kernel_size=par[i][13], activation=par[i][14]), input_shape=(None, n_steps, n_features))) \n",
        "  model.add(TimeDistributed(MaxPooling1D(pool_size=par[i][15])))\n",
        "  model.add(TimeDistributed(Flatten()))\n",
        "  model.add(LSTM(par[i][16], activation='tanh', dropout = par[i][17])) \n",
        "  model.add(Dense(1))\n",
        "  opt = keras.optimizers.Adam(learning_rate=par[i][18])\n",
        "  model.compile(optimizer=opt, loss='mse') \n",
        "  # fit model\n",
        "  history = model.fit(xtrain_CNN_lstm, ytrain, epochs=par[i][19], verbose=1,validation_split= 0.2,callbacks=[cp_callback, es_callback, lr_callback])\n",
        "  yhat = model.predict(xtest_CNN_lstm, verbose=0)\n",
        "  yhat_rescaled = yhat*risc_range + risc_min\n",
        "  #\n",
        "  yy = yhat_rescaled.reshape(yhat_rescaled.shape[0])\n",
        "  res = zone_accuracy(ytest_rescaled, yy, mode='clarke', detailed=False, diabetes_type=1)\n",
        "  clarke_results = clarke_results.append(pd.DataFrame(res), ignore_index=False, verify_integrity=False, sort=None)\n",
        "  a = max_error(ytest_rescaled, yy)\n",
        "  b = sqrt(mean_squared_error(ytest_rescaled, yhat_rescaled))\n",
        "  c = mean_absolute_error(ytest_rescaled, yy)\n",
        "  resarr = [a,b,c]\n",
        "  err_results = err_results.append(pd.DataFrame(resarr))\n",
        "  err_results.to_csv('err.csv')\n",
        "  clarke_results.to_csv('clk.csv') \n",
        "  !cp err.csv \"/content/drive/My Drive/Risultati_NN\"\n",
        "  !cp clk.csv \"/content/drive/My Drive/Risultati_NN\"\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0075 - val_loss: 0.0012\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0010\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0011\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 8.5215e-04\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 8.6195e-04\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 7.3498e-04\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0014 - val_loss: 8.9541e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 7.0053e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 6.8561e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 6.4239e-04\n",
            "Epoch 11/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 0.0012\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 6.0271e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 6.2596e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.3502e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.3483e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.3245e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.3140e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2510e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.3027e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2556e-04\n",
            "Epoch 21/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-10.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2154e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2154e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2154e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2154e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2154e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2154e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2154e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 31/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-13.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 41/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 0.0010    \n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999960041971e-16.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 51/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000036274937e-18.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 61/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-21.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 71/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-24.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 81/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 0.0010    \n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.0000001181490945e-27.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 91/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 0.0010    \n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.0000001235416983e-30.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 101/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1.000000097210625e-33.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 111/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.0000001155777242e-36.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 121/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1.0000001256222315e-39.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 131/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 1.0000002153053333e-42.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 141/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1.0005271035279193e-45.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 151/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 1.401298464324817e-48.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 161/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 171/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 181/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 191/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.2153e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0081 - val_loss: 0.0010\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0010\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 9.5565e-04\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0012\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 9.9808e-04\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 0.0010\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 7.8790e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 7.0019e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 6.5828e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 9.4641e-04\n",
            "Epoch 11/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 0.0012\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00011188889420332594.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 6.5149e-04\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.3413e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.9662e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.9649e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.5357e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.8630e-04 - val_loss: 5.3159e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9820e-04 - val_loss: 5.3037e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9408e-04 - val_loss: 6.4972e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.8874e-04 - val_loss: 5.4427e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.8037e-04 - val_loss: 5.8627e-04\n",
            "Epoch 21/200\n",
            "271/285 [===========================>..] - ETA: 0s - loss: 9.6834e-04\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.2519124438565793e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6640e-04 - val_loss: 5.1539e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5380e-04 - val_loss: 5.2103e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3804e-04 - val_loss: 5.4281e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4493e-04 - val_loss: 5.1236e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3073e-04 - val_loss: 5.3556e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4030e-04 - val_loss: 5.0757e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3700e-04 - val_loss: 5.8114e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3714e-04 - val_loss: 5.0600e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3816e-04 - val_loss: 5.1104e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3333e-04 - val_loss: 5.1934e-04\n",
            "Epoch 31/200\n",
            "269/285 [===========================>..] - ETA: 0s - loss: 9.0846e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.4007509345093341e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4111e-04 - val_loss: 5.2100e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2833e-04 - val_loss: 5.1544e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1881e-04 - val_loss: 5.2414e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3071e-04 - val_loss: 5.3206e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1691e-04 - val_loss: 5.2368e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2157e-04 - val_loss: 5.2645e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2618e-04 - val_loss: 5.1646e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2963e-04 - val_loss: 5.3509e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3427e-04 - val_loss: 5.1908e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3115e-04 - val_loss: 5.2966e-04\n",
            "Epoch 41/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 9.2893e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.567284717667563e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2506e-04 - val_loss: 5.1142e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2221e-04 - val_loss: 5.1638e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1061e-04 - val_loss: 5.1848e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1291e-04 - val_loss: 5.1987e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2290e-04 - val_loss: 5.2051e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2904e-04 - val_loss: 5.2064e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3671e-04 - val_loss: 5.2116e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2425e-04 - val_loss: 5.2142e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2229e-04 - val_loss: 5.2067e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1697e-04 - val_loss: 5.2078e-04\n",
            "Epoch 51/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 8.9215e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.7536174984419023e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2421e-04 - val_loss: 5.2037e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2535e-04 - val_loss: 5.2044e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2626e-04 - val_loss: 5.2051e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2160e-04 - val_loss: 5.2053e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2280e-04 - val_loss: 5.2038e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2295e-04 - val_loss: 5.2028e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2412e-04 - val_loss: 5.2049e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2436e-04 - val_loss: 5.2052e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2512e-04 - val_loss: 5.2056e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3309e-04 - val_loss: 5.2060e-04\n",
            "Epoch 61/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 9.3736e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.962103156098275e-09.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3359e-04 - val_loss: 5.2066e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2447e-04 - val_loss: 5.2066e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3303e-04 - val_loss: 5.2066e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3739e-04 - val_loss: 5.2066e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2322e-04 - val_loss: 5.2066e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2458e-04 - val_loss: 5.2066e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2800e-04 - val_loss: 5.2066e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3632e-04 - val_loss: 5.2067e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1963e-04 - val_loss: 5.2067e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3486e-04 - val_loss: 5.2067e-04\n",
            "Epoch 71/200\n",
            "270/285 [===========================>..] - ETA: 0s - loss: 9.3677e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 2.1953754096119362e-10.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2756e-04 - val_loss: 5.2067e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3329e-04 - val_loss: 5.2067e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2479e-04 - val_loss: 5.2067e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1285e-04 - val_loss: 5.2067e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2748e-04 - val_loss: 5.2067e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1898e-04 - val_loss: 5.2067e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1189e-04 - val_loss: 5.2067e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1067e-04 - val_loss: 5.2067e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2296e-04 - val_loss: 5.2067e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2414e-04 - val_loss: 5.2067e-04\n",
            "Epoch 81/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 9.4432e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 2.4563811523406168e-11.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4029e-04 - val_loss: 5.2067e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3156e-04 - val_loss: 5.2067e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3065e-04 - val_loss: 5.2067e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1999e-04 - val_loss: 5.2067e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2873e-04 - val_loss: 5.2067e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2636e-04 - val_loss: 5.2067e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2261e-04 - val_loss: 5.2067e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3373e-04 - val_loss: 5.2067e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1241e-04 - val_loss: 5.2067e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2481e-04 - val_loss: 5.2067e-04\n",
            "Epoch 91/200\n",
            "271/285 [===========================>..] - ETA: 0s - loss: 9.1738e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 2.7484175257808725e-12.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1517e-04 - val_loss: 5.2067e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2412e-04 - val_loss: 5.2067e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2374e-04 - val_loss: 5.2067e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2738e-04 - val_loss: 5.2067e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2023e-04 - val_loss: 5.2067e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3021e-04 - val_loss: 5.2067e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2136e-04 - val_loss: 5.2067e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2569e-04 - val_loss: 5.2067e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2109e-04 - val_loss: 5.2067e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2380e-04 - val_loss: 5.2067e-04\n",
            "Epoch 101/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 9.1848e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 3.0751739101248726e-13.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1410e-04 - val_loss: 5.2067e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3384e-04 - val_loss: 5.2067e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3524e-04 - val_loss: 5.2067e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3049e-04 - val_loss: 5.2067e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2789e-04 - val_loss: 5.2067e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3054e-04 - val_loss: 5.2067e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1795e-04 - val_loss: 5.2067e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2708e-04 - val_loss: 5.2067e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2603e-04 - val_loss: 5.2067e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2937e-04 - val_loss: 5.2067e-04\n",
            "Epoch 111/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 9.1798e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 3.4407778525843324e-14.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3283e-04 - val_loss: 5.2067e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2724e-04 - val_loss: 5.2067e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1946e-04 - val_loss: 5.2067e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2784e-04 - val_loss: 5.2067e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3499e-04 - val_loss: 5.2067e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1788e-04 - val_loss: 5.2067e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2235e-04 - val_loss: 5.2067e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2592e-04 - val_loss: 5.2067e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3921e-04 - val_loss: 5.2067e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2878e-04 - val_loss: 5.2067e-04\n",
            "Epoch 121/200\n",
            "271/285 [===========================>..] - ETA: 0s - loss: 9.3479e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 3.849848201733022e-15.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2411e-04 - val_loss: 5.2067e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1500e-04 - val_loss: 5.2067e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2948e-04 - val_loss: 5.2067e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2228e-04 - val_loss: 5.2067e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2217e-04 - val_loss: 5.2067e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2260e-04 - val_loss: 5.2067e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2993e-04 - val_loss: 5.2067e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2775e-04 - val_loss: 5.2067e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3323e-04 - val_loss: 5.2067e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1849e-04 - val_loss: 5.2067e-04\n",
            "Epoch 131/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 9.2904e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 4.307552350291347e-16.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2567e-04 - val_loss: 5.2067e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2644e-04 - val_loss: 5.2067e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1742e-04 - val_loss: 5.2067e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2672e-04 - val_loss: 5.2067e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2561e-04 - val_loss: 5.2067e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1925e-04 - val_loss: 5.2067e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3373e-04 - val_loss: 5.2067e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2486e-04 - val_loss: 5.2067e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3328e-04 - val_loss: 5.2067e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2648e-04 - val_loss: 5.2067e-04\n",
            "Epoch 141/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 9.2940e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 4.819672559664602e-17.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2640e-04 - val_loss: 5.2067e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2813e-04 - val_loss: 5.2067e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2690e-04 - val_loss: 5.2067e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2209e-04 - val_loss: 5.2067e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2106e-04 - val_loss: 5.2067e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2960e-04 - val_loss: 5.2067e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2914e-04 - val_loss: 5.2067e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2068e-04 - val_loss: 5.2067e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1720e-04 - val_loss: 5.2067e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1674e-04 - val_loss: 5.2067e-04\n",
            "Epoch 151/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 9.2037e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 5.3926778904803665e-18.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2457e-04 - val_loss: 5.2067e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1057e-04 - val_loss: 5.2067e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1408e-04 - val_loss: 5.2067e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1878e-04 - val_loss: 5.2067e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2523e-04 - val_loss: 5.2067e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2313e-04 - val_loss: 5.2067e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2935e-04 - val_loss: 5.2067e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2582e-04 - val_loss: 5.2067e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1954e-04 - val_loss: 5.2067e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2282e-04 - val_loss: 5.2067e-04\n",
            "Epoch 161/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 9.2654e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 6.033807337228358e-19.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2360e-04 - val_loss: 5.2067e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1735e-04 - val_loss: 5.2067e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2390e-04 - val_loss: 5.2067e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2706e-04 - val_loss: 5.2067e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2864e-04 - val_loss: 5.2067e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3179e-04 - val_loss: 5.2067e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2161e-04 - val_loss: 5.2067e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3144e-04 - val_loss: 5.2067e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2399e-04 - val_loss: 5.2067e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2336e-04 - val_loss: 5.2067e-04\n",
            "Epoch 171/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 9.3733e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 6.751160121007746e-20.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3078e-04 - val_loss: 5.2067e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1689e-04 - val_loss: 5.2067e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2609e-04 - val_loss: 5.2067e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3295e-04 - val_loss: 5.2067e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2395e-04 - val_loss: 5.2067e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2189e-04 - val_loss: 5.2067e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2653e-04 - val_loss: 5.2067e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2204e-04 - val_loss: 5.2067e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3263e-04 - val_loss: 5.2067e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2847e-04 - val_loss: 5.2067e-04\n",
            "Epoch 181/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 9.1638e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 7.553797842761685e-21.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1846e-04 - val_loss: 5.2067e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2151e-04 - val_loss: 5.2067e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3201e-04 - val_loss: 5.2067e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2100e-04 - val_loss: 5.2067e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4555e-04 - val_loss: 5.2067e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2450e-04 - val_loss: 5.2067e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2203e-04 - val_loss: 5.2067e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2418e-04 - val_loss: 5.2067e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3011e-04 - val_loss: 5.2067e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3389e-04 - val_loss: 5.2067e-04\n",
            "Epoch 191/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 9.2908e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 8.451860657551969e-22.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2720e-04 - val_loss: 5.2067e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1522e-04 - val_loss: 5.2067e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2642e-04 - val_loss: 5.2067e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2522e-04 - val_loss: 5.2067e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3147e-04 - val_loss: 5.2067e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2552e-04 - val_loss: 5.2067e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2164e-04 - val_loss: 5.2067e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2347e-04 - val_loss: 5.2067e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1583e-04 - val_loss: 5.2067e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2682e-04 - val_loss: 5.2067e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0087 - val_loss: 0.0012\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0010\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 9.9536e-04\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 8.9999e-04\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 8.3517e-04\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 8.0821e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 7.6201e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 9.1243e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 7.5260e-04\n",
            "Epoch 11/200\n",
            "272/285 [===========================>..] - ETA: 0s - loss: 0.0013\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00022277778835915442.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 6.8170e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 6.5464e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 6.7463e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 9.5103e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 6.0812e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.8374e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 6.8013e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.7439e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.5394e-04\n",
            "Epoch 21/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 9.9165e-04\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.9629940484818586e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9170e-04 - val_loss: 5.4717e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.7636e-04 - val_loss: 6.2461e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.7207e-04 - val_loss: 6.0474e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6521e-04 - val_loss: 5.3477e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5685e-04 - val_loss: 5.6192e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6174e-04 - val_loss: 5.3031e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6472e-04 - val_loss: 5.2391e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6087e-04 - val_loss: 5.2025e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5872e-04 - val_loss: 5.3306e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4037e-04 - val_loss: 5.4620e-04\n",
            "Epoch 31/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 9.4673e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.1056447720976494e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4594e-04 - val_loss: 5.1085e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4080e-04 - val_loss: 5.2280e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3686e-04 - val_loss: 5.4881e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4397e-04 - val_loss: 5.2461e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4177e-04 - val_loss: 5.2681e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1294e-04 - val_loss: 5.3756e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1840e-04 - val_loss: 5.1177e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3315e-04 - val_loss: 5.3546e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3605e-04 - val_loss: 5.2334e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2070e-04 - val_loss: 5.1822e-04\n",
            "Epoch 41/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 9.2280e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.4631308876147766e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2192e-04 - val_loss: 5.1061e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1534e-04 - val_loss: 5.2712e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1816e-04 - val_loss: 5.2096e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2077e-04 - val_loss: 5.3634e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1972e-04 - val_loss: 5.1780e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1916e-04 - val_loss: 5.1081e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1565e-04 - val_loss: 5.2442e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1876e-04 - val_loss: 5.3322e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2343e-04 - val_loss: 5.3371e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1640e-04 - val_loss: 5.1876e-04\n",
            "Epoch 51/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 9.2805e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 5.487308150501727e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3010e-04 - val_loss: 5.2461e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2162e-04 - val_loss: 5.3000e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1212e-04 - val_loss: 5.3281e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1373e-04 - val_loss: 5.2944e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2096e-04 - val_loss: 5.2704e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1254e-04 - val_loss: 5.2796e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1628e-04 - val_loss: 5.2371e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1389e-04 - val_loss: 5.2428e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2224e-04 - val_loss: 5.2756e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1815e-04 - val_loss: 5.2785e-04\n",
            "Epoch 61/200\n",
            "271/285 [===========================>..] - ETA: 0s - loss: 9.2622e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.2224503680929249e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1723e-04 - val_loss: 5.2966e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2385e-04 - val_loss: 5.2940e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2018e-04 - val_loss: 5.2818e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2881e-04 - val_loss: 5.2662e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2339e-04 - val_loss: 5.2692e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0464e-04 - val_loss: 5.2682e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1085e-04 - val_loss: 5.2616e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0577e-04 - val_loss: 5.2638e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2289e-04 - val_loss: 5.2585e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3092e-04 - val_loss: 5.2663e-04\n",
            "Epoch 71/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 9.2759e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 2.7233478524143114e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2381e-04 - val_loss: 5.2558e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1870e-04 - val_loss: 5.2593e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2306e-04 - val_loss: 5.2593e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2023e-04 - val_loss: 5.2597e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2654e-04 - val_loss: 5.2610e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2474e-04 - val_loss: 5.2611e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1665e-04 - val_loss: 5.2611e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0336e-04 - val_loss: 5.2616e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2054e-04 - val_loss: 5.2637e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2894e-04 - val_loss: 5.2629e-04\n",
            "Epoch 81/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 9.2633e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 6.067013642092118e-09.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2538e-04 - val_loss: 5.2621e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2825e-04 - val_loss: 5.2623e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1301e-04 - val_loss: 5.2625e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2305e-04 - val_loss: 5.2631e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1144e-04 - val_loss: 5.2627e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0542e-04 - val_loss: 5.2633e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1911e-04 - val_loss: 5.2635e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2162e-04 - val_loss: 5.2629e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2944e-04 - val_loss: 5.2632e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2634e-04 - val_loss: 5.2636e-04\n",
            "Epoch 91/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 9.0295e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.3515957982453604e-09.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0272e-04 - val_loss: 5.2635e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2592e-04 - val_loss: 5.2634e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1509e-04 - val_loss: 5.2634e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1460e-04 - val_loss: 5.2634e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2530e-04 - val_loss: 5.2634e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1995e-04 - val_loss: 5.2634e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1159e-04 - val_loss: 5.2635e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2227e-04 - val_loss: 5.2635e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2072e-04 - val_loss: 5.2635e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2486e-04 - val_loss: 5.2635e-04\n",
            "Epoch 101/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 9.1001e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 3.01105516631317e-10.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0511e-04 - val_loss: 5.2635e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2625e-04 - val_loss: 5.2635e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2006e-04 - val_loss: 5.2635e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0539e-04 - val_loss: 5.2635e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1780e-04 - val_loss: 5.2635e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2547e-04 - val_loss: 5.2635e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1719e-04 - val_loss: 5.2635e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1313e-04 - val_loss: 5.2635e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2613e-04 - val_loss: 5.2635e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2063e-04 - val_loss: 5.2635e-04\n",
            "Epoch 111/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 9.2543e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 6.707962071608422e-11.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2489e-04 - val_loss: 5.2635e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2653e-04 - val_loss: 5.2635e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2020e-04 - val_loss: 5.2635e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0876e-04 - val_loss: 5.2635e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0263e-04 - val_loss: 5.2635e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1318e-04 - val_loss: 5.2635e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1377e-04 - val_loss: 5.2635e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1485e-04 - val_loss: 5.2635e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1822e-04 - val_loss: 5.2635e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1932e-04 - val_loss: 5.2635e-04\n",
            "Epoch 121/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 9.3325e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1.4943848596842772e-11.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2890e-04 - val_loss: 5.2635e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1684e-04 - val_loss: 5.2635e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0778e-04 - val_loss: 5.2635e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2086e-04 - val_loss: 5.2635e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1671e-04 - val_loss: 5.2635e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3083e-04 - val_loss: 5.2635e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1605e-04 - val_loss: 5.2635e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2843e-04 - val_loss: 5.2635e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2581e-04 - val_loss: 5.2635e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1816e-04 - val_loss: 5.2635e-04\n",
            "Epoch 131/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 9.1264e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 3.3291573028430367e-12.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1431e-04 - val_loss: 5.2635e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2132e-04 - val_loss: 5.2635e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2565e-04 - val_loss: 5.2635e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2177e-04 - val_loss: 5.2635e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1722e-04 - val_loss: 5.2635e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2437e-04 - val_loss: 5.2635e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1740e-04 - val_loss: 5.2635e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2105e-04 - val_loss: 5.2635e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1466e-04 - val_loss: 5.2635e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1301e-04 - val_loss: 5.2635e-04\n",
            "Epoch 141/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 9.2387e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 7.416622567826825e-13.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2212e-04 - val_loss: 5.2635e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2752e-04 - val_loss: 5.2635e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1659e-04 - val_loss: 5.2635e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1393e-04 - val_loss: 5.2635e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2054e-04 - val_loss: 5.2635e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2169e-04 - val_loss: 5.2635e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1049e-04 - val_loss: 5.2635e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2246e-04 - val_loss: 5.2635e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1167e-04 - val_loss: 5.2635e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1928e-04 - val_loss: 5.2635e-04\n",
            "Epoch 151/200\n",
            "272/285 [===========================>..] - ETA: 0s - loss: 9.1920e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 1.652258713599868e-13.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1727e-04 - val_loss: 5.2635e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2603e-04 - val_loss: 5.2635e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1117e-04 - val_loss: 5.2635e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0934e-04 - val_loss: 5.2635e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1307e-04 - val_loss: 5.2635e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1630e-04 - val_loss: 5.2635e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1574e-04 - val_loss: 5.2635e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2042e-04 - val_loss: 5.2635e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1597e-04 - val_loss: 5.2635e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2551e-04 - val_loss: 5.2635e-04\n",
            "Epoch 161/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 9.0641e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 3.6808651084269984e-14.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1631e-04 - val_loss: 5.2635e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2084e-04 - val_loss: 5.2635e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1379e-04 - val_loss: 5.2635e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1413e-04 - val_loss: 5.2635e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1746e-04 - val_loss: 5.2635e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2793e-04 - val_loss: 5.2635e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2680e-04 - val_loss: 5.2635e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1496e-04 - val_loss: 5.2635e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0781e-04 - val_loss: 5.2635e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2071e-04 - val_loss: 5.2635e-04\n",
            "Epoch 171/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 9.1027e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 8.200149295303135e-15.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1139e-04 - val_loss: 5.2635e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2145e-04 - val_loss: 5.2635e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0976e-04 - val_loss: 5.2635e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1583e-04 - val_loss: 5.2635e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1557e-04 - val_loss: 5.2635e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0634e-04 - val_loss: 5.2635e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2107e-04 - val_loss: 5.2635e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1278e-04 - val_loss: 5.2635e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1842e-04 - val_loss: 5.2635e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2415e-04 - val_loss: 5.2635e-04\n",
            "Epoch 181/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 9.2447e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 1.8268111229976963e-15.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1447e-04 - val_loss: 5.2635e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1063e-04 - val_loss: 5.2635e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0902e-04 - val_loss: 5.2635e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2430e-04 - val_loss: 5.2635e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1662e-04 - val_loss: 5.2635e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1525e-04 - val_loss: 5.2635e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2541e-04 - val_loss: 5.2635e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2472e-04 - val_loss: 5.2635e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1365e-04 - val_loss: 5.2635e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1839e-04 - val_loss: 5.2635e-04\n",
            "Epoch 191/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 9.1613e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 4.06972940432498e-16.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1783e-04 - val_loss: 5.2635e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0971e-04 - val_loss: 5.2635e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2420e-04 - val_loss: 5.2635e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0787e-04 - val_loss: 5.2635e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2124e-04 - val_loss: 5.2635e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2036e-04 - val_loss: 5.2635e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2203e-04 - val_loss: 5.2635e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1726e-04 - val_loss: 5.2635e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2036e-04 - val_loss: 5.2635e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1955e-04 - val_loss: 5.2635e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 2s 6ms/step - loss: 0.0090 - val_loss: 0.0011\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 9.7963e-04\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 8.5038e-04\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0011\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 7.7987e-04\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 6.7261e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 8.1654e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 8.5117e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 5.5866e-04\n",
            "Epoch 11/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 0.0012\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00033366668251498294.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 6.0503e-04\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 8.3353e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.8080e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.4137e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 7.2934e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.7976e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.4499e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.8355e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.1265e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9846e-04 - val_loss: 5.6954e-04\n",
            "Epoch 21/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001113334505741174.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.6720e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4267e-04 - val_loss: 5.0553e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4274e-04 - val_loss: 5.0362e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3740e-04 - val_loss: 4.7648e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3297e-04 - val_loss: 5.0703e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3924e-04 - val_loss: 5.2402e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2576e-04 - val_loss: 4.7695e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3355e-04 - val_loss: 5.0809e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2343e-04 - val_loss: 6.5558e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2014e-04 - val_loss: 4.9353e-04\n",
            "Epoch 31/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 9.1859e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.714826150341348e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1749e-04 - val_loss: 5.0316e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0533e-04 - val_loss: 4.7058e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9510e-04 - val_loss: 5.8306e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8911e-04 - val_loss: 5.2748e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8382e-04 - val_loss: 4.6511e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8796e-04 - val_loss: 4.7126e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8718e-04 - val_loss: 4.6256e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8408e-04 - val_loss: 4.6674e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8651e-04 - val_loss: 4.7536e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8026e-04 - val_loss: 4.6661e-04\n",
            "Epoch 41/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 8.9206e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.2395136140791388e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8894e-04 - val_loss: 4.6247e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7736e-04 - val_loss: 4.8746e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8289e-04 - val_loss: 4.8488e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6427e-04 - val_loss: 4.7823e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7132e-04 - val_loss: 4.9787e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7963e-04 - val_loss: 4.6615e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6923e-04 - val_loss: 4.9339e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7456e-04 - val_loss: 4.8361e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7127e-04 - val_loss: 5.0454e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7587e-04 - val_loss: 4.6775e-04\n",
            "Epoch 51/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.7612e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 4.135843663486109e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7298e-04 - val_loss: 4.7127e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7602e-04 - val_loss: 4.6919e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7371e-04 - val_loss: 4.7179e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7137e-04 - val_loss: 4.8051e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5561e-04 - val_loss: 4.7904e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7360e-04 - val_loss: 4.7890e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6616e-04 - val_loss: 4.5877e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6122e-04 - val_loss: 4.8201e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6926e-04 - val_loss: 4.7598e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6297e-04 - val_loss: 4.7459e-04\n",
            "Epoch 61/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 8.7958e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.379993108760876e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7485e-04 - val_loss: 4.8795e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6230e-04 - val_loss: 4.7526e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6936e-04 - val_loss: 4.7444e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6339e-04 - val_loss: 4.7778e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7055e-04 - val_loss: 4.7993e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7234e-04 - val_loss: 4.7673e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7390e-04 - val_loss: 4.7346e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7044e-04 - val_loss: 4.7366e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6717e-04 - val_loss: 4.9353e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5914e-04 - val_loss: 4.7683e-04\n",
            "Epoch 71/200\n",
            "272/285 [===========================>..] - ETA: 0s - loss: 8.8069e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 4.604577099295663e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7252e-04 - val_loss: 4.8230e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6415e-04 - val_loss: 4.7915e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5127e-04 - val_loss: 4.7558e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6962e-04 - val_loss: 4.8058e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6780e-04 - val_loss: 4.7891e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6744e-04 - val_loss: 4.8514e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6553e-04 - val_loss: 4.7994e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6432e-04 - val_loss: 4.7965e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6148e-04 - val_loss: 4.7634e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6737e-04 - val_loss: 4.7753e-04\n",
            "Epoch 81/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 8.7062e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.5363938544510346e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6311e-04 - val_loss: 4.7938e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6863e-04 - val_loss: 4.7679e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6727e-04 - val_loss: 4.7771e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6347e-04 - val_loss: 4.7676e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5598e-04 - val_loss: 4.7688e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6549e-04 - val_loss: 4.7728e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8015e-04 - val_loss: 4.7776e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7255e-04 - val_loss: 4.7749e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5952e-04 - val_loss: 4.7695e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6469e-04 - val_loss: 4.7737e-04\n",
            "Epoch 91/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 8.6958e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 5.126434345944138e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6757e-04 - val_loss: 4.7705e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4819e-04 - val_loss: 4.7685e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5735e-04 - val_loss: 4.7698e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6506e-04 - val_loss: 4.7749e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5397e-04 - val_loss: 4.7729e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7237e-04 - val_loss: 4.7720e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7023e-04 - val_loss: 4.7738e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7029e-04 - val_loss: 4.7790e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6686e-04 - val_loss: 4.7797e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5709e-04 - val_loss: 4.7843e-04\n",
            "Epoch 101/200\n",
            "272/285 [===========================>..] - ETA: 0s - loss: 8.7641e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1.7105202741637033e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6431e-04 - val_loss: 4.7841e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7016e-04 - val_loss: 4.7837e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6739e-04 - val_loss: 4.7830e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6300e-04 - val_loss: 4.7836e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7611e-04 - val_loss: 4.7830e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6214e-04 - val_loss: 4.7831e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6170e-04 - val_loss: 4.7831e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5782e-04 - val_loss: 4.7840e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5518e-04 - val_loss: 4.7824e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6438e-04 - val_loss: 4.7831e-04\n",
            "Epoch 111/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 8.6040e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 5.707435930091265e-09.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5788e-04 - val_loss: 4.7816e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7263e-04 - val_loss: 4.7817e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5299e-04 - val_loss: 4.7813e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6230e-04 - val_loss: 4.7811e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5734e-04 - val_loss: 4.7814e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6179e-04 - val_loss: 4.7811e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6155e-04 - val_loss: 4.7810e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5818e-04 - val_loss: 4.7815e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7049e-04 - val_loss: 4.7811e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6997e-04 - val_loss: 4.7808e-04\n",
            "Epoch 121/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 8.6114e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1.904381097705965e-09.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5815e-04 - val_loss: 4.7808e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5137e-04 - val_loss: 4.7808e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6669e-04 - val_loss: 4.7808e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6721e-04 - val_loss: 4.7809e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6158e-04 - val_loss: 4.7808e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5815e-04 - val_loss: 4.7809e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6141e-04 - val_loss: 4.7809e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5638e-04 - val_loss: 4.7809e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5985e-04 - val_loss: 4.7809e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6864e-04 - val_loss: 4.7809e-04\n",
            "Epoch 131/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 8.5812e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 6.354284927863792e-10.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5678e-04 - val_loss: 4.7809e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7251e-04 - val_loss: 4.7809e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7989e-04 - val_loss: 4.7809e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6727e-04 - val_loss: 4.7809e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6691e-04 - val_loss: 4.7809e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6516e-04 - val_loss: 4.7809e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6491e-04 - val_loss: 4.7809e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6783e-04 - val_loss: 4.7809e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6346e-04 - val_loss: 4.7809e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6591e-04 - val_loss: 4.7809e-04\n",
            "Epoch 141/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 8.5833e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 2.120213053643146e-10.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5833e-04 - val_loss: 4.7809e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6830e-04 - val_loss: 4.7809e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7046e-04 - val_loss: 4.7809e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6951e-04 - val_loss: 4.7809e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6195e-04 - val_loss: 4.7809e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6641e-04 - val_loss: 4.7809e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5762e-04 - val_loss: 4.7809e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6741e-04 - val_loss: 4.7809e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6424e-04 - val_loss: 4.7809e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6096e-04 - val_loss: 4.7809e-04\n",
            "Epoch 151/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 8.6319e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 7.074444153173005e-11.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6294e-04 - val_loss: 4.7809e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5540e-04 - val_loss: 4.7809e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7403e-04 - val_loss: 4.7809e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6782e-04 - val_loss: 4.7809e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6636e-04 - val_loss: 4.7809e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6952e-04 - val_loss: 4.7809e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6255e-04 - val_loss: 4.7809e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6709e-04 - val_loss: 4.7809e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6798e-04 - val_loss: 4.7809e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6555e-04 - val_loss: 4.7809e-04\n",
            "Epoch 161/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 8.6701e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 2.3605060841166057e-11.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5942e-04 - val_loss: 4.7809e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6901e-04 - val_loss: 4.7809e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5885e-04 - val_loss: 4.7809e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6682e-04 - val_loss: 4.7809e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5879e-04 - val_loss: 4.7809e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7423e-04 - val_loss: 4.7809e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6563e-04 - val_loss: 4.7809e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5956e-04 - val_loss: 4.7809e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7646e-04 - val_loss: 4.7809e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7572e-04 - val_loss: 4.7809e-04\n",
            "Epoch 171/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 8.5321e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 7.87622189170334e-12.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5321e-04 - val_loss: 4.7809e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6903e-04 - val_loss: 4.7809e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6454e-04 - val_loss: 4.7809e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6117e-04 - val_loss: 4.7809e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5071e-04 - val_loss: 4.7809e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5759e-04 - val_loss: 4.7809e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6689e-04 - val_loss: 4.7809e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6793e-04 - val_loss: 4.7809e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5403e-04 - val_loss: 4.7809e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6605e-04 - val_loss: 4.7809e-04\n",
            "Epoch 181/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 8.6881e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 2.628032822032019e-12.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6773e-04 - val_loss: 4.7809e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6001e-04 - val_loss: 4.7809e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6434e-04 - val_loss: 4.7809e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5774e-04 - val_loss: 4.7809e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7074e-04 - val_loss: 4.7809e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6645e-04 - val_loss: 4.7809e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7107e-04 - val_loss: 4.7809e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6632e-04 - val_loss: 4.7809e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6414e-04 - val_loss: 4.7809e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5812e-04 - val_loss: 4.7809e-04\n",
            "Epoch 191/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.8653e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 8.768869356040135e-13.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8332e-04 - val_loss: 4.7809e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6632e-04 - val_loss: 4.7809e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7161e-04 - val_loss: 4.7809e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6102e-04 - val_loss: 4.7809e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6100e-04 - val_loss: 4.7809e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6315e-04 - val_loss: 4.7809e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7237e-04 - val_loss: 4.7809e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7079e-04 - val_loss: 4.7809e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6707e-04 - val_loss: 4.7809e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5712e-04 - val_loss: 4.7809e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 2s 5ms/step - loss: 0.0080 - val_loss: 0.0010\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 9.9502e-04\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0011\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0011\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 8.0135e-04\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 8.6541e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 8.0288e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0013 - val_loss: 6.7398e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0013 - val_loss: 7.9386e-04\n",
            "Epoch 11/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 0.0012\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0004445555766708114.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 5.4535e-04\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 9.9480e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.1554e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 5.1188e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 4.9817e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.7595e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9426e-04 - val_loss: 5.9757e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.0062e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 7.0576e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9600e-04 - val_loss: 4.7038e-04\n",
            "Epoch 21/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 9.8068e-04\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00019762964742322866.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.7390e-04 - val_loss: 4.5055e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4430e-04 - val_loss: 4.5204e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2280e-04 - val_loss: 4.6813e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2130e-04 - val_loss: 4.8056e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0636e-04 - val_loss: 4.8474e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0896e-04 - val_loss: 0.0013\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1645e-04 - val_loss: 5.8212e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2197e-04 - val_loss: 4.8437e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9784e-04 - val_loss: 5.2917e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1066e-04 - val_loss: 4.3830e-04\n",
            "Epoch 31/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 9.1409e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 8.785735797331047e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1321e-04 - val_loss: 4.5581e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9163e-04 - val_loss: 4.5841e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7270e-04 - val_loss: 4.4773e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7799e-04 - val_loss: 7.0809e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8351e-04 - val_loss: 5.2779e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7694e-04 - val_loss: 4.9365e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7304e-04 - val_loss: 4.8947e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7813e-04 - val_loss: 4.6874e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6492e-04 - val_loss: 5.5792e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6991e-04 - val_loss: 6.3191e-04\n",
            "Epoch 41/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.8574e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.905747550096647e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8254e-04 - val_loss: 4.9129e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6623e-04 - val_loss: 4.4843e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7508e-04 - val_loss: 4.2809e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5228e-04 - val_loss: 4.8021e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7087e-04 - val_loss: 4.9520e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6514e-04 - val_loss: 4.5558e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5871e-04 - val_loss: 4.6069e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5794e-04 - val_loss: 5.1003e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5294e-04 - val_loss: 4.8920e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5040e-04 - val_loss: 4.9174e-04\n",
            "Epoch 51/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 8.5606e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.7363217531965348e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5526e-04 - val_loss: 4.5940e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6096e-04 - val_loss: 4.7015e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5056e-04 - val_loss: 4.8398e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4895e-04 - val_loss: 4.4842e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6216e-04 - val_loss: 4.4586e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3770e-04 - val_loss: 4.3560e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5925e-04 - val_loss: 4.7224e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2970e-04 - val_loss: 4.9416e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4295e-04 - val_loss: 4.8931e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4538e-04 - val_loss: 4.3614e-04\n",
            "Epoch 61/200\n",
            "272/285 [===========================>..] - ETA: 0s - loss: 8.3471e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 7.718914989204171e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3817e-04 - val_loss: 4.4095e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4166e-04 - val_loss: 4.7178e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4265e-04 - val_loss: 4.6194e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4060e-04 - val_loss: 4.6493e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.3673e-04 - val_loss: 4.5382e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3550e-04 - val_loss: 4.5074e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5425e-04 - val_loss: 4.5487e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3852e-04 - val_loss: 4.6111e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5172e-04 - val_loss: 4.4581e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4613e-04 - val_loss: 4.6003e-04\n",
            "Epoch 71/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 8.5094e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 3.4314866284653865e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4845e-04 - val_loss: 4.6447e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3074e-04 - val_loss: 4.4358e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2885e-04 - val_loss: 4.4710e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4715e-04 - val_loss: 4.5664e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4688e-04 - val_loss: 4.4607e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3754e-04 - val_loss: 4.6159e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5169e-04 - val_loss: 4.5671e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4086e-04 - val_loss: 4.6599e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5037e-04 - val_loss: 4.6161e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4186e-04 - val_loss: 4.4410e-04\n",
            "Epoch 81/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 8.3806e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.5254864652989554e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3678e-04 - val_loss: 4.5716e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3393e-04 - val_loss: 4.4742e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3033e-04 - val_loss: 4.5143e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3862e-04 - val_loss: 4.5304e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3546e-04 - val_loss: 4.4844e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3838e-04 - val_loss: 4.5010e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3654e-04 - val_loss: 4.5381e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5243e-04 - val_loss: 4.5882e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4057e-04 - val_loss: 4.5039e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3837e-04 - val_loss: 4.5830e-04\n",
            "Epoch 91/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 8.4076e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 6.781634905084704e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4117e-04 - val_loss: 4.5099e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4285e-04 - val_loss: 4.5267e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2908e-04 - val_loss: 4.5250e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3932e-04 - val_loss: 4.5128e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4310e-04 - val_loss: 4.5604e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3532e-04 - val_loss: 4.4950e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3859e-04 - val_loss: 4.5520e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3966e-04 - val_loss: 4.5430e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4158e-04 - val_loss: 4.4973e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3780e-04 - val_loss: 4.5477e-04\n",
            "Epoch 101/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 8.4727e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 3.0148133986232196e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4040e-04 - val_loss: 4.5084e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4362e-04 - val_loss: 4.5187e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3083e-04 - val_loss: 4.5375e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3672e-04 - val_loss: 4.5530e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4143e-04 - val_loss: 4.5589e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4065e-04 - val_loss: 4.5295e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4372e-04 - val_loss: 4.5436e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3446e-04 - val_loss: 4.5434e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3907e-04 - val_loss: 4.5443e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3429e-04 - val_loss: 4.5440e-04\n",
            "Epoch 111/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 8.2909e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.3402520908635374e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2774e-04 - val_loss: 4.5330e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5253e-04 - val_loss: 4.5299e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2265e-04 - val_loss: 4.5356e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3845e-04 - val_loss: 4.5348e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2898e-04 - val_loss: 4.5342e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3894e-04 - val_loss: 4.5326e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5180e-04 - val_loss: 4.5348e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3626e-04 - val_loss: 4.5322e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3645e-04 - val_loss: 4.5333e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3763e-04 - val_loss: 4.5251e-04\n",
            "Epoch 121/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 8.5241e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 5.958165348513628e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5278e-04 - val_loss: 4.5276e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4273e-04 - val_loss: 4.5306e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4133e-04 - val_loss: 4.5318e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4794e-04 - val_loss: 4.5335e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3566e-04 - val_loss: 4.5324e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3996e-04 - val_loss: 4.5363e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2586e-04 - val_loss: 4.5369e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4613e-04 - val_loss: 4.5328e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3949e-04 - val_loss: 4.5362e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3517e-04 - val_loss: 4.5395e-04\n",
            "Epoch 131/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 8.5326e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 2.6487355812873956e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4723e-04 - val_loss: 4.5369e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4530e-04 - val_loss: 4.5384e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4546e-04 - val_loss: 4.5378e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4096e-04 - val_loss: 4.5372e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3659e-04 - val_loss: 4.5360e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4157e-04 - val_loss: 4.5373e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4842e-04 - val_loss: 4.5382e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4513e-04 - val_loss: 4.5381e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3964e-04 - val_loss: 4.5387e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3984e-04 - val_loss: 4.5362e-04\n",
            "Epoch 141/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 8.4648e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1.1775101398825427e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4234e-04 - val_loss: 4.5353e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3724e-04 - val_loss: 4.5351e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4496e-04 - val_loss: 4.5352e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3429e-04 - val_loss: 4.5341e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3504e-04 - val_loss: 4.5340e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4366e-04 - val_loss: 4.5343e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3426e-04 - val_loss: 4.5350e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3343e-04 - val_loss: 4.5354e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5964e-04 - val_loss: 4.5356e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4162e-04 - val_loss: 4.5355e-04\n",
            "Epoch 151/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 8.3722e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 5.234686914650726e-09.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3406e-04 - val_loss: 4.5358e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3815e-04 - val_loss: 4.5357e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4458e-04 - val_loss: 4.5359e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3842e-04 - val_loss: 4.5359e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4120e-04 - val_loss: 4.5362e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3397e-04 - val_loss: 4.5363e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5457e-04 - val_loss: 4.5363e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3003e-04 - val_loss: 4.5363e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4695e-04 - val_loss: 4.5363e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4245e-04 - val_loss: 4.5363e-04\n",
            "Epoch 161/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 8.4077e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 2.3271091457289906e-09.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3974e-04 - val_loss: 4.5364e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3982e-04 - val_loss: 4.5364e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3697e-04 - val_loss: 4.5364e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4032e-04 - val_loss: 4.5364e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4351e-04 - val_loss: 4.5364e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3928e-04 - val_loss: 4.5364e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4242e-04 - val_loss: 4.5364e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4450e-04 - val_loss: 4.5365e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3768e-04 - val_loss: 4.5365e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2982e-04 - val_loss: 4.5365e-04\n",
            "Epoch 171/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 8.3989e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 1.0345293108097546e-09.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3911e-04 - val_loss: 4.5365e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3289e-04 - val_loss: 4.5365e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3445e-04 - val_loss: 4.5365e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4630e-04 - val_loss: 4.5365e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5023e-04 - val_loss: 4.5365e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3890e-04 - val_loss: 4.5365e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3580e-04 - val_loss: 4.5365e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3936e-04 - val_loss: 4.5365e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4148e-04 - val_loss: 4.5365e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4116e-04 - val_loss: 4.5365e-04\n",
            "Epoch 181/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 8.3555e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 4.599057704051606e-10.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3555e-04 - val_loss: 4.5365e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4404e-04 - val_loss: 4.5365e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4007e-04 - val_loss: 4.5365e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3945e-04 - val_loss: 4.5365e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3844e-04 - val_loss: 4.5365e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3989e-04 - val_loss: 4.5365e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3350e-04 - val_loss: 4.5365e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4832e-04 - val_loss: 4.5365e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4771e-04 - val_loss: 4.5365e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4226e-04 - val_loss: 4.5365e-04\n",
            "Epoch 191/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 8.4269e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 2.044536695870273e-10.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4089e-04 - val_loss: 4.5365e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3554e-04 - val_loss: 4.5365e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2892e-04 - val_loss: 4.5365e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5666e-04 - val_loss: 4.5365e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3231e-04 - val_loss: 4.5365e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3994e-04 - val_loss: 4.5365e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3292e-04 - val_loss: 4.5365e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3233e-04 - val_loss: 4.5365e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2634e-04 - val_loss: 4.5365e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4111e-04 - val_loss: 4.5365e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 2s 6ms/step - loss: 0.0081 - val_loss: 0.0010\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0011\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0011\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 9.5407e-04\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 9.5979e-04\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 7.7658e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 7.9023e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 8.7476e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 6.7189e-04\n",
            "Epoch 11/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 0.0013\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005554444708266398.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 6.7344e-04\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 6.1063e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 6.8845e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 6.3323e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.0920e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.3939e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.8858e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 4.9752e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5881e-04 - val_loss: 4.6858e-04\n",
            "Epoch 21/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 9.6048e-04\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0003085185342739957.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6234e-04 - val_loss: 4.6041e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1710e-04 - val_loss: 4.6662e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3058e-04 - val_loss: 4.8270e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4590e-04 - val_loss: 5.2542e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2311e-04 - val_loss: 4.5486e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1633e-04 - val_loss: 5.4633e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0939e-04 - val_loss: 4.3136e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1998e-04 - val_loss: 4.5636e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1829e-04 - val_loss: 4.9909e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0801e-04 - val_loss: 5.0410e-04\n",
            "Epoch 31/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.7981e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00017136490394154355.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8194e-04 - val_loss: 4.5259e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8237e-04 - val_loss: 6.9235e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7683e-04 - val_loss: 4.3440e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7999e-04 - val_loss: 4.4120e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7131e-04 - val_loss: 4.3344e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7118e-04 - val_loss: 4.7371e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6467e-04 - val_loss: 4.2742e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5470e-04 - val_loss: 4.3644e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6626e-04 - val_loss: 4.4658e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5765e-04 - val_loss: 6.0414e-04\n",
            "Epoch 41/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 8.6902e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.518368766778925e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6810e-04 - val_loss: 4.3192e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5923e-04 - val_loss: 4.3598e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5197e-04 - val_loss: 4.4554e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5358e-04 - val_loss: 4.8254e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3893e-04 - val_loss: 4.5031e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5099e-04 - val_loss: 4.8260e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4419e-04 - val_loss: 4.2361e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5151e-04 - val_loss: 5.2060e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4683e-04 - val_loss: 5.6683e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4385e-04 - val_loss: 4.7790e-04\n",
            "Epoch 51/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.5289e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 5.286924881673056e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5049e-04 - val_loss: 4.4216e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3968e-04 - val_loss: 4.5706e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3198e-04 - val_loss: 4.2032e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3652e-04 - val_loss: 4.4938e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3990e-04 - val_loss: 4.3298e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2771e-04 - val_loss: 4.9034e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2794e-04 - val_loss: 4.2911e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1564e-04 - val_loss: 4.3096e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1876e-04 - val_loss: 4.3674e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2018e-04 - val_loss: 4.4391e-04\n",
            "Epoch 61/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 8.4639e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.9365929614868944e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4267e-04 - val_loss: 4.5510e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2324e-04 - val_loss: 4.9074e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2161e-04 - val_loss: 4.3532e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2382e-04 - val_loss: 5.3888e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1922e-04 - val_loss: 4.3855e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2134e-04 - val_loss: 4.1650e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2075e-04 - val_loss: 4.2231e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1932e-04 - val_loss: 4.3426e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2451e-04 - val_loss: 4.2842e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2132e-04 - val_loss: 4.3742e-04\n",
            "Epoch 71/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 8.0871e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.6311142929326808e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0508e-04 - val_loss: 4.6512e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1489e-04 - val_loss: 4.2829e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2780e-04 - val_loss: 4.4795e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1240e-04 - val_loss: 4.4033e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1087e-04 - val_loss: 4.2781e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2695e-04 - val_loss: 5.0136e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2169e-04 - val_loss: 4.6669e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2419e-04 - val_loss: 4.5066e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1585e-04 - val_loss: 4.1696e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2627e-04 - val_loss: 4.1102e-04\n",
            "Epoch 81/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 8.1284e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 9.059934069632113e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1284e-04 - val_loss: 4.2900e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0862e-04 - val_loss: 4.3581e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1909e-04 - val_loss: 4.2128e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1431e-04 - val_loss: 4.3399e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1484e-04 - val_loss: 4.3941e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0770e-04 - val_loss: 4.1582e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2196e-04 - val_loss: 4.4503e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1641e-04 - val_loss: 4.3223e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2016e-04 - val_loss: 4.3542e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2076e-04 - val_loss: 4.3910e-04\n",
            "Epoch 91/200\n",
            "272/285 [===========================>..] - ETA: 0s - loss: 8.2064e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 5.032290118530606e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1512e-04 - val_loss: 4.4907e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1425e-04 - val_loss: 4.3137e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1336e-04 - val_loss: 4.4032e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1995e-04 - val_loss: 4.2076e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1189e-04 - val_loss: 4.4066e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1277e-04 - val_loss: 4.5044e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0904e-04 - val_loss: 4.4298e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0703e-04 - val_loss: 4.3032e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2153e-04 - val_loss: 4.3863e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2421e-04 - val_loss: 4.5821e-04\n",
            "Epoch 101/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 8.1505e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 2.7951575893388913e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1556e-04 - val_loss: 4.3808e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1430e-04 - val_loss: 4.4608e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1457e-04 - val_loss: 4.3458e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0906e-04 - val_loss: 4.2808e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2500e-04 - val_loss: 4.3049e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1901e-04 - val_loss: 4.2900e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1700e-04 - val_loss: 4.3054e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1106e-04 - val_loss: 4.3116e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2101e-04 - val_loss: 4.2759e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0660e-04 - val_loss: 4.5014e-04\n",
            "Epoch 111/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.1225e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.5525547437644288e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1222e-04 - val_loss: 4.2617e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1621e-04 - val_loss: 4.3240e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1587e-04 - val_loss: 4.3162e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0998e-04 - val_loss: 4.3365e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1225e-04 - val_loss: 4.3154e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0975e-04 - val_loss: 4.4029e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0677e-04 - val_loss: 4.3272e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1600e-04 - val_loss: 4.3049e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0666e-04 - val_loss: 4.4984e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1136e-04 - val_loss: 4.3724e-04\n",
            "Epoch 121/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 8.1170e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 8.623579103894169e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1119e-04 - val_loss: 4.3245e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1253e-04 - val_loss: 4.3779e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0771e-04 - val_loss: 4.3614e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1427e-04 - val_loss: 4.3172e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0638e-04 - val_loss: 4.3832e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0427e-04 - val_loss: 4.3128e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1659e-04 - val_loss: 4.3614e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1421e-04 - val_loss: 4.3476e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0357e-04 - val_loss: 4.3495e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1659e-04 - val_loss: 4.3424e-04\n",
            "Epoch 131/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.1385e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 4.789919168754547e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0850e-04 - val_loss: 4.3019e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1010e-04 - val_loss: 4.3402e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1439e-04 - val_loss: 4.3466e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0258e-04 - val_loss: 4.3492e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1866e-04 - val_loss: 4.3593e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0792e-04 - val_loss: 4.3283e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0874e-04 - val_loss: 4.3128e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0595e-04 - val_loss: 4.3318e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1524e-04 - val_loss: 4.3709e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9672e-04 - val_loss: 4.3533e-04\n",
            "Epoch 141/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.1664e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 2.6605340183196956e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1266e-04 - val_loss: 4.3325e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0338e-04 - val_loss: 4.3337e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1374e-04 - val_loss: 4.3437e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1093e-04 - val_loss: 4.3542e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0276e-04 - val_loss: 4.3402e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1194e-04 - val_loss: 4.3457e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0902e-04 - val_loss: 4.3574e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1429e-04 - val_loss: 4.3377e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1011e-04 - val_loss: 4.3470e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0964e-04 - val_loss: 4.3461e-04\n",
            "Epoch 151/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.0286e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 1.4777788577981077e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1313e-04 - val_loss: 4.3413e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0719e-04 - val_loss: 4.3444e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1207e-04 - val_loss: 4.3460e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1024e-04 - val_loss: 4.3429e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1485e-04 - val_loss: 4.3416e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1263e-04 - val_loss: 4.3383e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0891e-04 - val_loss: 4.3425e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1317e-04 - val_loss: 4.3361e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0245e-04 - val_loss: 4.3479e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1514e-04 - val_loss: 4.3520e-04\n",
            "Epoch 161/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 8.1404e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 8.208240377724803e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1043e-04 - val_loss: 4.3559e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0687e-04 - val_loss: 4.3476e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0364e-04 - val_loss: 4.3472e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0894e-04 - val_loss: 4.3499e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0115e-04 - val_loss: 4.3455e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0625e-04 - val_loss: 4.3426e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1051e-04 - val_loss: 4.3470e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0850e-04 - val_loss: 4.3479e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0790e-04 - val_loss: 4.3485e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1371e-04 - val_loss: 4.3468e-04\n",
            "Epoch 171/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 8.1671e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 4.559221380004727e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1694e-04 - val_loss: 4.3439e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1529e-04 - val_loss: 4.3413e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0027e-04 - val_loss: 4.3449e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1389e-04 - val_loss: 4.3428e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0104e-04 - val_loss: 4.3437e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0612e-04 - val_loss: 4.3430e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0433e-04 - val_loss: 4.3423e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1226e-04 - val_loss: 4.3464e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1053e-04 - val_loss: 4.3466e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0245e-04 - val_loss: 4.3469e-04\n",
            "Epoch 181/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.1781e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 2.5323941238516074e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1818e-04 - val_loss: 4.3480e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0792e-04 - val_loss: 4.3477e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1671e-04 - val_loss: 4.3477e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2434e-04 - val_loss: 4.3470e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1224e-04 - val_loss: 4.3465e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0836e-04 - val_loss: 4.3468e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0271e-04 - val_loss: 4.3467e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0381e-04 - val_loss: 4.3467e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1395e-04 - val_loss: 4.3476e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1957e-04 - val_loss: 4.3467e-04\n",
            "Epoch 191/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.1250e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 1.4066042925580606e-08.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1083e-04 - val_loss: 4.3456e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1727e-04 - val_loss: 4.3463e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1566e-04 - val_loss: 4.3464e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0567e-04 - val_loss: 4.3449e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0912e-04 - val_loss: 4.3447e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2348e-04 - val_loss: 4.3445e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1222e-04 - val_loss: 4.3446e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0728e-04 - val_loss: 4.3444e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2199e-04 - val_loss: 4.3448e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1472e-04 - val_loss: 4.3446e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 2s 5ms/step - loss: 0.0079 - val_loss: 0.0011\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0011\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 8.5594e-04\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0011\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 8.5468e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 9.3279e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 7.4903e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0013 - val_loss: 6.6558e-04\n",
            "Epoch 11/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 0.0012\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0006663333649824684.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 9.2554e-04\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 6.8178e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 7.3860e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 7.4985e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.7323e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 5.7041e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 3s 11ms/step - loss: 0.0010 - val_loss: 6.1657e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 3s 9ms/step - loss: 0.0010 - val_loss: 5.8386e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.8718e-04 - val_loss: 0.0011\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.9647e-04 - val_loss: 4.8660e-04\n",
            "Epoch 21/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0004440001305192709.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 6.2963e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3661e-04 - val_loss: 6.7123e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3443e-04 - val_loss: 8.5734e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4645e-04 - val_loss: 6.1963e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4857e-04 - val_loss: 4.4935e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4525e-04 - val_loss: 4.5285e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3297e-04 - val_loss: 4.7387e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3013e-04 - val_loss: 4.4266e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3328e-04 - val_loss: 4.3827e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2875e-04 - val_loss: 5.2962e-04\n",
            "Epoch 31/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 9.3103e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002958520807636281.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3095e-04 - val_loss: 4.3689e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0160e-04 - val_loss: 9.3915e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0215e-04 - val_loss: 4.4931e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9403e-04 - val_loss: 4.9054e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9205e-04 - val_loss: 5.6930e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9263e-04 - val_loss: 6.3974e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8612e-04 - val_loss: 4.3530e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2648e-04 - val_loss: 4.4631e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9419e-04 - val_loss: 4.5630e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9649e-04 - val_loss: 4.4156e-04\n",
            "Epoch 41/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 8.7504e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00019713610139054555.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7848e-04 - val_loss: 4.4012e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9164e-04 - val_loss: 4.3123e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8091e-04 - val_loss: 4.8509e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6407e-04 - val_loss: 4.2324e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6090e-04 - val_loss: 4.2638e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6455e-04 - val_loss: 4.2350e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8987e-04 - val_loss: 4.2904e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7418e-04 - val_loss: 6.2332e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7725e-04 - val_loss: 4.7925e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8167e-04 - val_loss: 4.7439e-04\n",
            "Epoch 51/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 8.7384e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00013135835814561384.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7050e-04 - val_loss: 4.3769e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4633e-04 - val_loss: 4.3120e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7018e-04 - val_loss: 5.2294e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6159e-04 - val_loss: 4.5680e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3800e-04 - val_loss: 4.7293e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5066e-04 - val_loss: 4.2557e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5602e-04 - val_loss: 4.2084e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4498e-04 - val_loss: 4.2363e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3599e-04 - val_loss: 5.2966e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6542e-04 - val_loss: 4.5128e-04\n",
            "Epoch 61/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 8.5517e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 8.752845616739554e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5014e-04 - val_loss: 4.2891e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2096e-04 - val_loss: 4.1786e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3076e-04 - val_loss: 4.9041e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3146e-04 - val_loss: 4.1821e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2554e-04 - val_loss: 4.2722e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5226e-04 - val_loss: 4.9980e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3950e-04 - val_loss: 4.9985e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3093e-04 - val_loss: 4.1138e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3834e-04 - val_loss: 4.3867e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2881e-04 - val_loss: 4.4990e-04\n",
            "Epoch 71/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 8.3416e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 5.832312594914887e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3136e-04 - val_loss: 4.1609e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.3073e-04 - val_loss: 4.3787e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1607e-04 - val_loss: 4.5274e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2558e-04 - val_loss: 5.1137e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2991e-04 - val_loss: 4.0886e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2177e-04 - val_loss: 4.2478e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2864e-04 - val_loss: 4.2509e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2181e-04 - val_loss: 4.2247e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2454e-04 - val_loss: 4.9750e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1604e-04 - val_loss: 4.2493e-04\n",
            "Epoch 81/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 8.2521e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 3.886264343964285e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1983e-04 - val_loss: 4.1910e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1374e-04 - val_loss: 4.3542e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1298e-04 - val_loss: 4.5389e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1754e-04 - val_loss: 4.1725e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1754e-04 - val_loss: 5.3100e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1051e-04 - val_loss: 4.1791e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0988e-04 - val_loss: 4.0769e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1404e-04 - val_loss: 4.2922e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0882e-04 - val_loss: 4.1427e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0875e-04 - val_loss: 4.4102e-04\n",
            "Epoch 91/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 8.1807e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 2.589547453438475e-05.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1983e-04 - val_loss: 4.0482e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1263e-04 - val_loss: 4.3665e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1315e-04 - val_loss: 4.4726e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1585e-04 - val_loss: 4.2907e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1183e-04 - val_loss: 4.4186e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1249e-04 - val_loss: 4.3787e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0474e-04 - val_loss: 4.1481e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0955e-04 - val_loss: 4.2253e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9890e-04 - val_loss: 4.6250e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0763e-04 - val_loss: 4.2537e-04\n",
            "Epoch 101/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 8.0998e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1.7255018254218158e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1074e-04 - val_loss: 4.6745e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0452e-04 - val_loss: 4.3683e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0156e-04 - val_loss: 4.2115e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0113e-04 - val_loss: 4.0827e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0105e-04 - val_loss: 4.3506e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0869e-04 - val_loss: 4.2814e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0932e-04 - val_loss: 4.3150e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0975e-04 - val_loss: 4.3938e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0334e-04 - val_loss: 4.2543e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9629e-04 - val_loss: 4.2745e-04\n",
            "Epoch 111/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 8.0326e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.1497594310033795e-05.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1397e-04 - val_loss: 4.5669e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9594e-04 - val_loss: 4.4154e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1103e-04 - val_loss: 4.2674e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0641e-04 - val_loss: 4.3817e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0254e-04 - val_loss: 4.2046e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9444e-04 - val_loss: 4.1731e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9776e-04 - val_loss: 4.4509e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0013e-04 - val_loss: 4.5159e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0594e-04 - val_loss: 4.2282e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9956e-04 - val_loss: 4.4045e-04\n",
            "Epoch 121/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 8.0696e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 7.661230562916898e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0377e-04 - val_loss: 4.7908e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9951e-04 - val_loss: 4.3128e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0810e-04 - val_loss: 4.5297e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9525e-04 - val_loss: 4.2594e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9779e-04 - val_loss: 4.4278e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9394e-04 - val_loss: 4.2523e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0305e-04 - val_loss: 4.1881e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9632e-04 - val_loss: 4.1821e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9511e-04 - val_loss: 4.3016e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1337e-04 - val_loss: 4.3077e-04\n",
            "Epoch 131/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 7.9856e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 5.10493324569931e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9856e-04 - val_loss: 4.3849e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9493e-04 - val_loss: 4.2411e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8913e-04 - val_loss: 4.2802e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9449e-04 - val_loss: 4.2351e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0814e-04 - val_loss: 4.2948e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9366e-04 - val_loss: 4.3071e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1096e-04 - val_loss: 4.3269e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9426e-04 - val_loss: 4.2365e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9362e-04 - val_loss: 4.3664e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0630e-04 - val_loss: 4.2008e-04\n",
            "Epoch 141/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 7.9623e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 3.401587208675968e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8949e-04 - val_loss: 4.3472e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9460e-04 - val_loss: 4.1982e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9194e-04 - val_loss: 4.2061e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8730e-04 - val_loss: 4.1870e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9685e-04 - val_loss: 4.3169e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9306e-04 - val_loss: 4.3776e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9620e-04 - val_loss: 4.3629e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9418e-04 - val_loss: 4.1324e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9539e-04 - val_loss: 4.2405e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0665e-04 - val_loss: 4.3221e-04\n",
            "Epoch 151/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 7.9527e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 2.2665909804497156e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9498e-04 - val_loss: 4.2791e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9731e-04 - val_loss: 4.2959e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0018e-04 - val_loss: 4.2973e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9550e-04 - val_loss: 4.2483e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9561e-04 - val_loss: 4.2423e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0164e-04 - val_loss: 4.2737e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9609e-04 - val_loss: 4.2561e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9160e-04 - val_loss: 4.2322e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9662e-04 - val_loss: 4.2936e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9414e-04 - val_loss: 4.2825e-04\n",
            "Epoch 161/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 7.9005e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 1.5103051909287993e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9026e-04 - val_loss: 4.3781e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9777e-04 - val_loss: 4.3277e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9309e-04 - val_loss: 4.2497e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8792e-04 - val_loss: 4.3777e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9825e-04 - val_loss: 4.2283e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9492e-04 - val_loss: 4.2442e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0261e-04 - val_loss: 4.2921e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9870e-04 - val_loss: 4.2727e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8668e-04 - val_loss: 4.2953e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8621e-04 - val_loss: 4.2685e-04\n",
            "Epoch 171/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.0622e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 1.0063666959088854e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0183e-04 - val_loss: 4.3625e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0250e-04 - val_loss: 4.2756e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9711e-04 - val_loss: 4.2998e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0199e-04 - val_loss: 4.2569e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9890e-04 - val_loss: 4.2802e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9588e-04 - val_loss: 4.2522e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9076e-04 - val_loss: 4.2706e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9683e-04 - val_loss: 4.2861e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9174e-04 - val_loss: 4.2389e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9667e-04 - val_loss: 4.2721e-04\n",
            "Epoch 181/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 7.9819e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 6.70575669611632e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9381e-04 - val_loss: 4.3149e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9034e-04 - val_loss: 4.2771e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9083e-04 - val_loss: 4.2795e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0490e-04 - val_loss: 4.2791e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0262e-04 - val_loss: 4.3019e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9715e-04 - val_loss: 4.2754e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9230e-04 - val_loss: 4.2754e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9317e-04 - val_loss: 4.2856e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0260e-04 - val_loss: 4.3242e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9391e-04 - val_loss: 4.2480e-04\n",
            "Epoch 191/200\n",
            "270/285 [===========================>..] - ETA: 0s - loss: 7.2726e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 4.468269235834062e-07.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9914e-04 - val_loss: 4.2788e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0413e-04 - val_loss: 4.2791e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0191e-04 - val_loss: 4.2816e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8873e-04 - val_loss: 4.2664e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0590e-04 - val_loss: 4.2489e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9771e-04 - val_loss: 4.2888e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0494e-04 - val_loss: 4.2781e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0561e-04 - val_loss: 4.2850e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9091e-04 - val_loss: 4.2828e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9984e-04 - val_loss: 4.2812e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 2s 6ms/step - loss: 0.0065 - val_loss: 0.0010\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0018 - val_loss: 0.0010\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0018 - val_loss: 9.9714e-04\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 9.2025e-04\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0010\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 9.3424e-04\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 7.0382e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0013 - val_loss: 7.1645e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 6.7097e-04\n",
            "Epoch 11/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 0.0012\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0007772222591382968.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 6.5415e-04\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.9199e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.5582e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.0795e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 4.9657e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 5.2634e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 4.6911e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.8926e-04 - val_loss: 5.5890e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 4.9896e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 6.1175e-04\n",
            "Epoch 21/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 9.8991e-04\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006040744232207846.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.8780e-04 - val_loss: 4.5657e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.8219e-04 - val_loss: 4.7339e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9877e-04 - val_loss: 0.0012\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.8564e-04 - val_loss: 5.1025e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.6468e-04 - val_loss: 4.7284e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.7440e-04 - val_loss: 4.5470e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3721e-04 - val_loss: 4.4089e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.4622e-04 - val_loss: 6.4444e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6428e-04 - val_loss: 5.2549e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4548e-04 - val_loss: 4.4600e-04\n",
            "Epoch 31/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 9.6037e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00046950008513198954.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.5969e-04 - val_loss: 5.0391e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3435e-04 - val_loss: 4.3974e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.5290e-04 - val_loss: 6.7908e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.1929e-04 - val_loss: 6.7574e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3158e-04 - val_loss: 4.6225e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2749e-04 - val_loss: 4.9140e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2946e-04 - val_loss: 4.6117e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.0617e-04 - val_loss: 4.3211e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.1177e-04 - val_loss: 4.3422e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0868e-04 - val_loss: 4.8146e-04\n",
            "Epoch 41/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 9.0869e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.000364905897489128.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.0723e-04 - val_loss: 4.8137e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0972e-04 - val_loss: 6.0690e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.0082e-04 - val_loss: 4.8793e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.8449e-04 - val_loss: 4.5378e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.8507e-04 - val_loss: 5.2178e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.9753e-04 - val_loss: 8.0739e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0484e-04 - val_loss: 4.9045e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.8171e-04 - val_loss: 4.2992e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9278e-04 - val_loss: 4.2728e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8046e-04 - val_loss: 4.3260e-04\n",
            "Epoch 51/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 8.7811e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0002836129687784705.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7811e-04 - val_loss: 5.5214e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.7487e-04 - val_loss: 4.2626e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6137e-04 - val_loss: 4.1461e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5636e-04 - val_loss: 4.3401e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8730e-04 - val_loss: 4.2164e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7288e-04 - val_loss: 5.2299e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6196e-04 - val_loss: 4.8000e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6849e-04 - val_loss: 4.1928e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.5993e-04 - val_loss: 7.9896e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6185e-04 - val_loss: 4.5622e-04\n",
            "Epoch 61/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 8.6491e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00022043029268388635.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6491e-04 - val_loss: 4.3597e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4223e-04 - val_loss: 4.1074e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5164e-04 - val_loss: 4.1731e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4308e-04 - val_loss: 4.3945e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2578e-04 - val_loss: 4.1428e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4322e-04 - val_loss: 4.1434e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3933e-04 - val_loss: 4.2560e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3970e-04 - val_loss: 4.5035e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3998e-04 - val_loss: 4.5495e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2541e-04 - val_loss: 4.0414e-04\n",
            "Epoch 71/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.4537e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0001713233177401384.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.4091e-04 - val_loss: 5.2583e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2962e-04 - val_loss: 4.9030e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2512e-04 - val_loss: 4.4945e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1613e-04 - val_loss: 7.7393e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0847e-04 - val_loss: 4.0631e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1862e-04 - val_loss: 5.2802e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4370e-04 - val_loss: 4.9811e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1412e-04 - val_loss: 4.5186e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2396e-04 - val_loss: 4.8370e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2602e-04 - val_loss: 4.1021e-04\n",
            "Epoch 81/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 8.1302e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0001331562925851257.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1205e-04 - val_loss: 4.2001e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1901e-04 - val_loss: 4.4069e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2064e-04 - val_loss: 4.7203e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1441e-04 - val_loss: 4.2785e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1713e-04 - val_loss: 4.5964e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0638e-04 - val_loss: 4.3296e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1337e-04 - val_loss: 4.1869e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1327e-04 - val_loss: 4.2383e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0680e-04 - val_loss: 4.5945e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1839e-04 - val_loss: 4.1633e-04\n",
            "Epoch 91/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.1771e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0001034920324471184.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1199e-04 - val_loss: 4.2919e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0554e-04 - val_loss: 4.2574e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0942e-04 - val_loss: 4.1959e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0893e-04 - val_loss: 4.3872e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0053e-04 - val_loss: 4.3507e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9959e-04 - val_loss: 4.6620e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9054e-04 - val_loss: 4.3387e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0054e-04 - val_loss: 4.1144e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0290e-04 - val_loss: 4.3179e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0115e-04 - val_loss: 4.0321e-04\n",
            "Epoch 101/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.0634e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 8.043630952064025e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0595e-04 - val_loss: 3.9473e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9221e-04 - val_loss: 4.1067e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9603e-04 - val_loss: 5.1404e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9728e-04 - val_loss: 4.5703e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9964e-04 - val_loss: 4.0276e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9917e-04 - val_loss: 4.0180e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8931e-04 - val_loss: 4.2166e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9953e-04 - val_loss: 4.0859e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7430e-04 - val_loss: 4.3825e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9364e-04 - val_loss: 4.7162e-04\n",
            "Epoch 111/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 7.8229e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 6.251688609255425e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8147e-04 - val_loss: 4.2485e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7805e-04 - val_loss: 4.3810e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9287e-04 - val_loss: 4.3193e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8149e-04 - val_loss: 4.5358e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8406e-04 - val_loss: 4.1857e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8361e-04 - val_loss: 4.0414e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9693e-04 - val_loss: 3.9313e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8091e-04 - val_loss: 4.1514e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7845e-04 - val_loss: 4.1930e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9057e-04 - val_loss: 3.9458e-04\n",
            "Epoch 121/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 7.9009e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 4.858951087953755e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8641e-04 - val_loss: 4.3237e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8617e-04 - val_loss: 4.5293e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7208e-04 - val_loss: 4.0839e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7734e-04 - val_loss: 3.9759e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8790e-04 - val_loss: 4.8185e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7072e-04 - val_loss: 4.0710e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8809e-04 - val_loss: 4.0165e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8606e-04 - val_loss: 3.9462e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7348e-04 - val_loss: 4.3455e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7359e-04 - val_loss: 4.2298e-04\n",
            "Epoch 131/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 7.7610e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 3.776484748110912e-05.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7645e-04 - val_loss: 3.9082e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7389e-04 - val_loss: 3.9802e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8473e-04 - val_loss: 4.8112e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6658e-04 - val_loss: 4.0426e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8017e-04 - val_loss: 4.3639e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8282e-04 - val_loss: 4.0991e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8068e-04 - val_loss: 4.2568e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7998e-04 - val_loss: 3.9584e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7441e-04 - val_loss: 4.1315e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7758e-04 - val_loss: 4.5835e-04\n",
            "Epoch 141/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 7.8261e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 2.9351678026109257e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8066e-04 - val_loss: 3.9913e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7188e-04 - val_loss: 3.9758e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7826e-04 - val_loss: 4.0416e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7631e-04 - val_loss: 4.0500e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7768e-04 - val_loss: 4.0207e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7844e-04 - val_loss: 4.1825e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7097e-04 - val_loss: 4.1699e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7125e-04 - val_loss: 4.2465e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7448e-04 - val_loss: 4.1269e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7314e-04 - val_loss: 4.0049e-04\n",
            "Epoch 151/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 7.6424e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 2.2812776927215357e-05.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6159e-04 - val_loss: 4.0297e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6869e-04 - val_loss: 4.0265e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.5862e-04 - val_loss: 4.1130e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6763e-04 - val_loss: 4.0180e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6353e-04 - val_loss: 4.0566e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7462e-04 - val_loss: 3.9424e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6796e-04 - val_loss: 3.9820e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7235e-04 - val_loss: 4.6488e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6886e-04 - val_loss: 3.9883e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6563e-04 - val_loss: 3.9353e-04\n",
            "Epoch 161/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 7.6830e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 1.7730597536582434e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6256e-04 - val_loss: 3.9774e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7883e-04 - val_loss: 4.0559e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7048e-04 - val_loss: 4.1353e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6932e-04 - val_loss: 4.2511e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7786e-04 - val_loss: 4.0216e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5883e-04 - val_loss: 4.0942e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6855e-04 - val_loss: 3.9955e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7024e-04 - val_loss: 4.0426e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6158e-04 - val_loss: 4.1497e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6603e-04 - val_loss: 4.1890e-04\n",
            "Epoch 171/200\n",
            "271/285 [===========================>..] - ETA: 0s - loss: 7.7363e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 1.3780614306395161e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6642e-04 - val_loss: 4.1406e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7730e-04 - val_loss: 3.9650e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7247e-04 - val_loss: 3.9932e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6293e-04 - val_loss: 4.1789e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6716e-04 - val_loss: 4.4361e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6774e-04 - val_loss: 4.3674e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6470e-04 - val_loss: 4.1594e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6936e-04 - val_loss: 4.2794e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6115e-04 - val_loss: 3.9335e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6486e-04 - val_loss: 4.0133e-04\n",
            "Epoch 181/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 7.8783e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 1.071059990964487e-05.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8345e-04 - val_loss: 3.9464e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7375e-04 - val_loss: 4.1108e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6736e-04 - val_loss: 4.0040e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6404e-04 - val_loss: 4.0186e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7176e-04 - val_loss: 4.0790e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6259e-04 - val_loss: 4.0667e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6979e-04 - val_loss: 4.1143e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6229e-04 - val_loss: 4.2369e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6573e-04 - val_loss: 3.9983e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6007e-04 - val_loss: 4.1395e-04\n",
            "Epoch 191/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 7.6870e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 8.324516163358768e-06.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6870e-04 - val_loss: 4.4133e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6765e-04 - val_loss: 4.2433e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6356e-04 - val_loss: 4.1366e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6157e-04 - val_loss: 4.0572e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6435e-04 - val_loss: 4.0313e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7550e-04 - val_loss: 4.1090e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5900e-04 - val_loss: 4.2206e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6458e-04 - val_loss: 3.9967e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6410e-04 - val_loss: 3.9912e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.4752e-04 - val_loss: 4.2065e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 2s 6ms/step - loss: 0.0083 - val_loss: 0.0013\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0018 - val_loss: 0.0011\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 9.1923e-04\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0012\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 7.7237e-04\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 6.9691e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 9.1716e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 6.1927e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 8.4005e-04\n",
            "Epoch 11/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 0.0011\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0008881111532941254.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 8.5940e-04\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 6.1922e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 7.2142e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 8.2341e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.7863e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 5.3413e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 5.1573e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 4.6585e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 6.8859e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 7.3412e-04\n",
            "Epoch 21/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 9.8322e-04\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0007887413606836667.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.9264e-04 - val_loss: 4.5510e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.8738e-04 - val_loss: 4.7517e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.2622e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.8862e-04 - val_loss: 4.4644e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9550e-04 - val_loss: 4.6442e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9709e-04 - val_loss: 6.1734e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6790e-04 - val_loss: 6.0636e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6256e-04 - val_loss: 7.7880e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5108e-04 - val_loss: 4.8507e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3337e-04 - val_loss: 4.6109e-04\n",
            "Epoch 31/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 9.7444e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0007004899457161728.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6532e-04 - val_loss: 6.1551e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.8587e-04 - val_loss: 5.0681e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2979e-04 - val_loss: 8.4883e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5831e-04 - val_loss: 5.0789e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0174e-04 - val_loss: 0.0012\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4192e-04 - val_loss: 6.0546e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5088e-04 - val_loss: 9.5621e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.4340e-04 - val_loss: 5.5316e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3142e-04 - val_loss: 5.6688e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1705e-04 - val_loss: 4.3418e-04\n",
            "Epoch 41/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 9.2533e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006221129230301206.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2437e-04 - val_loss: 4.5943e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3352e-04 - val_loss: 5.1472e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2070e-04 - val_loss: 4.7960e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3960e-04 - val_loss: 6.1627e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.1811e-04 - val_loss: 7.1205e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1109e-04 - val_loss: 4.5357e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5071e-04 - val_loss: 6.1658e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.1765e-04 - val_loss: 4.2694e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3271e-04 - val_loss: 4.8590e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9416e-04 - val_loss: 4.7451e-04\n",
            "Epoch 51/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 9.3130e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0005525054181602576.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2837e-04 - val_loss: 4.8101e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9981e-04 - val_loss: 5.2136e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1387e-04 - val_loss: 4.3452e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7129e-04 - val_loss: 4.2692e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6839e-04 - val_loss: 5.4474e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7189e-04 - val_loss: 4.2589e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9538e-04 - val_loss: 5.1025e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7852e-04 - val_loss: 4.2871e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6980e-04 - val_loss: 4.4641e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0600e-04 - val_loss: 5.0235e-04\n",
            "Epoch 61/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.7466e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00049068621077135.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7308e-04 - val_loss: 4.8573e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6633e-04 - val_loss: 5.4870e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6818e-04 - val_loss: 5.8223e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6725e-04 - val_loss: 4.6706e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6847e-04 - val_loss: 4.3801e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7552e-04 - val_loss: 4.3406e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7341e-04 - val_loss: 0.0010\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9050e-04 - val_loss: 6.3264e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.4768e-04 - val_loss: 4.1730e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.8253e-04 - val_loss: 4.3239e-04\n",
            "Epoch 71/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 8.6492e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0004357838804329124.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6545e-04 - val_loss: 4.3074e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6721e-04 - val_loss: 4.3121e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6775e-04 - val_loss: 4.8754e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5668e-04 - val_loss: 4.2252e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4201e-04 - val_loss: 4.6230e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.5502e-04 - val_loss: 4.1176e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3248e-04 - val_loss: 4.1267e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.4512e-04 - val_loss: 4.3023e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4614e-04 - val_loss: 6.5752e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7889e-04 - val_loss: 4.3048e-04\n",
            "Epoch 81/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 8.6516e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.00038702450324005133.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6691e-04 - val_loss: 5.0481e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5751e-04 - val_loss: 4.2093e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.4042e-04 - val_loss: 4.0592e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.3233e-04 - val_loss: 4.6505e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5860e-04 - val_loss: 6.3207e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3928e-04 - val_loss: 4.6592e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2116e-04 - val_loss: 5.3352e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5580e-04 - val_loss: 4.8703e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3867e-04 - val_loss: 4.1806e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4520e-04 - val_loss: 4.0126e-04\n",
            "Epoch 91/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 8.4579e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0003437207700432433.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4546e-04 - val_loss: 4.1487e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2596e-04 - val_loss: 5.2033e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1441e-04 - val_loss: 5.5269e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1788e-04 - val_loss: 4.1028e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2408e-04 - val_loss: 3.9754e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2287e-04 - val_loss: 6.2126e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2506e-04 - val_loss: 4.6375e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1741e-04 - val_loss: 6.6155e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0596e-04 - val_loss: 4.3757e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2290e-04 - val_loss: 4.2606e-04\n",
            "Epoch 101/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 8.0976e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.00030526224196526325.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0844e-04 - val_loss: 4.4562e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0283e-04 - val_loss: 4.4096e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0395e-04 - val_loss: 4.0640e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4191e-04 - val_loss: 3.9887e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1512e-04 - val_loss: 4.1285e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1418e-04 - val_loss: 4.3813e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9931e-04 - val_loss: 4.0712e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1612e-04 - val_loss: 4.0698e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0309e-04 - val_loss: 4.1160e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0733e-04 - val_loss: 5.1671e-04\n",
            "Epoch 111/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 8.0282e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.0002711067949001315.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9996e-04 - val_loss: 5.1261e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1902e-04 - val_loss: 4.5714e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0006e-04 - val_loss: 6.5004e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1233e-04 - val_loss: 4.2703e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0604e-04 - val_loss: 4.9129e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0245e-04 - val_loss: 4.2134e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9807e-04 - val_loss: 4.1092e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9643e-04 - val_loss: 4.0147e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8290e-04 - val_loss: 4.2805e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9283e-04 - val_loss: 4.9743e-04\n",
            "Epoch 121/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 8.1406e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 0.00024077296867229355.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1256e-04 - val_loss: 5.0077e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8434e-04 - val_loss: 4.1731e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0869e-04 - val_loss: 4.3234e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8371e-04 - val_loss: 4.0077e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1277e-04 - val_loss: 4.3666e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0894e-04 - val_loss: 4.3615e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9826e-04 - val_loss: 3.9441e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9112e-04 - val_loss: 4.4607e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1042e-04 - val_loss: 7.9804e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9134e-04 - val_loss: 3.9881e-04\n",
            "Epoch 131/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 7.8817e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0002138331433137258.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8451e-04 - val_loss: 4.1654e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8555e-04 - val_loss: 3.9846e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8386e-04 - val_loss: 4.7359e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9128e-04 - val_loss: 4.0105e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9271e-04 - val_loss: 4.5254e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8134e-04 - val_loss: 3.9355e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8537e-04 - val_loss: 4.2462e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8201e-04 - val_loss: 4.0686e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.8649e-04 - val_loss: 4.0275e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6744e-04 - val_loss: 3.9027e-04\n",
            "Epoch 141/200\n",
            "278/285 [============================>.] - ETA: 0s - loss: 7.6685e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 0.00018990759415383863.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6586e-04 - val_loss: 4.0163e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6918e-04 - val_loss: 4.0395e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8590e-04 - val_loss: 4.8213e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7850e-04 - val_loss: 4.2217e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6847e-04 - val_loss: 4.0829e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7385e-04 - val_loss: 3.8817e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8288e-04 - val_loss: 4.0520e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7645e-04 - val_loss: 4.4516e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6919e-04 - val_loss: 3.9729e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6981e-04 - val_loss: 3.9921e-04\n",
            "Epoch 151/200\n",
            "276/285 [============================>.] - ETA: 0s - loss: 7.9122e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.00016865903801064835.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.8265e-04 - val_loss: 4.3401e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6198e-04 - val_loss: 5.0024e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6761e-04 - val_loss: 3.9161e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7735e-04 - val_loss: 4.2451e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6798e-04 - val_loss: 4.1738e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7504e-04 - val_loss: 4.4584e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6988e-04 - val_loss: 4.0768e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6969e-04 - val_loss: 4.7081e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7768e-04 - val_loss: 4.3844e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6735e-04 - val_loss: 4.5107e-04\n",
            "Epoch 161/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 7.7873e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 0.00014978796772872254.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7898e-04 - val_loss: 4.1865e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8017e-04 - val_loss: 5.0022e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6398e-04 - val_loss: 3.9967e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6829e-04 - val_loss: 4.1474e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6390e-04 - val_loss: 3.9557e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6285e-04 - val_loss: 3.8863e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6654e-04 - val_loss: 4.6943e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.7231e-04 - val_loss: 3.9015e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6663e-04 - val_loss: 3.9900e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6603e-04 - val_loss: 4.2269e-04\n",
            "Epoch 171/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 7.6922e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.00013302836150493627.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6223e-04 - val_loss: 4.0039e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5954e-04 - val_loss: 4.4243e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7527e-04 - val_loss: 3.9858e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.5279e-04 - val_loss: 3.9327e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6480e-04 - val_loss: 4.5465e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6516e-04 - val_loss: 3.8307e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5391e-04 - val_loss: 5.7812e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6717e-04 - val_loss: 4.3743e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6509e-04 - val_loss: 4.0526e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5203e-04 - val_loss: 4.0229e-04\n",
            "Epoch 181/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 7.6961e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 0.00011814396085780269.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6378e-04 - val_loss: 4.6456e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5615e-04 - val_loss: 3.9241e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5824e-04 - val_loss: 3.8967e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.6093e-04 - val_loss: 5.0463e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.4659e-04 - val_loss: 4.6949e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5257e-04 - val_loss: 4.3401e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.6191e-04 - val_loss: 3.9648e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.4894e-04 - val_loss: 4.1754e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.7810e-04 - val_loss: 4.8218e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.5243e-04 - val_loss: 3.9317e-04\n",
            "Epoch 191/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 7.6015e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 0.00010492496215576668.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5645e-04 - val_loss: 3.8960e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.4438e-04 - val_loss: 3.8962e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.5042e-04 - val_loss: 4.7692e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.5980e-04 - val_loss: 4.8545e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.4155e-04 - val_loss: 3.9013e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5037e-04 - val_loss: 4.6652e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.4850e-04 - val_loss: 4.6921e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.4780e-04 - val_loss: 3.8286e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.4674e-04 - val_loss: 4.3387e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.5241e-04 - val_loss: 3.9257e-04\n",
            "Epoch 1/200\n",
            "285/285 [==============================] - 2s 6ms/step - loss: 0.0083 - val_loss: 0.0011\n",
            "Epoch 2/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0011\n",
            "Epoch 3/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 9.2591e-04\n",
            "Epoch 4/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0012\n",
            "Epoch 5/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 9.5529e-04\n",
            "Epoch 6/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0014 - val_loss: 7.7422e-04\n",
            "Epoch 7/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 7.8561e-04\n",
            "Epoch 8/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 6.5313e-04\n",
            "Epoch 9/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 6.6927e-04\n",
            "Epoch 10/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 5.8480e-04\n",
            "Epoch 11/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 0.0011\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009990000474499538.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 7.6697e-04\n",
            "Epoch 12/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 7.8427e-04\n",
            "Epoch 13/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.0712e-04\n",
            "Epoch 14/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 8.5887e-04\n",
            "Epoch 15/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 5.8977e-04\n",
            "Epoch 16/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 7.0935e-04\n",
            "Epoch 17/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.6788e-04\n",
            "Epoch 18/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9374e-04 - val_loss: 4.7531e-04\n",
            "Epoch 19/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.8612e-04 - val_loss: 7.1468e-04\n",
            "Epoch 20/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6459e-04 - val_loss: 4.5551e-04\n",
            "Epoch 21/200\n",
            "284/285 [============================>.] - ETA: 0s - loss: 0.0010\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.000998001039843075.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 4.9919e-04\n",
            "Epoch 22/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.3019e-04\n",
            "Epoch 23/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.6218e-04 - val_loss: 6.1488e-04\n",
            "Epoch 24/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.9585e-04 - val_loss: 5.6535e-04\n",
            "Epoch 25/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.1285e-04\n",
            "Epoch 26/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3021e-04 - val_loss: 5.5070e-04\n",
            "Epoch 27/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 4.5559e-04\n",
            "Epoch 28/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4674e-04 - val_loss: 4.5331e-04\n",
            "Epoch 29/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6203e-04 - val_loss: 6.1264e-04\n",
            "Epoch 30/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2682e-04 - val_loss: 4.9196e-04\n",
            "Epoch 31/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 9.3659e-04\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009970030789263546.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3659e-04 - val_loss: 7.0786e-04\n",
            "Epoch 32/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 5.6144e-04\n",
            "Epoch 33/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3349e-04 - val_loss: 6.0068e-04\n",
            "Epoch 34/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3519e-04 - val_loss: 5.5945e-04\n",
            "Epoch 35/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6193e-04 - val_loss: 6.0050e-04\n",
            "Epoch 36/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3883e-04 - val_loss: 7.6828e-04\n",
            "Epoch 37/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.5510e-04 - val_loss: 4.5779e-04\n",
            "Epoch 38/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.8656e-04 - val_loss: 5.1579e-04\n",
            "Epoch 39/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6921e-04 - val_loss: 4.5798e-04\n",
            "Epoch 40/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1861e-04 - val_loss: 4.7691e-04\n",
            "Epoch 41/200\n",
            "280/285 [============================>.] - ETA: 0s - loss: 9.3649e-04\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0009960060484008864.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3561e-04 - val_loss: 4.5633e-04\n",
            "Epoch 42/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.5602e-04 - val_loss: 4.4106e-04\n",
            "Epoch 43/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2787e-04 - val_loss: 4.1818e-04\n",
            "Epoch 44/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4798e-04 - val_loss: 4.4940e-04\n",
            "Epoch 45/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4522e-04 - val_loss: 4.3311e-04\n",
            "Epoch 46/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3239e-04 - val_loss: 5.0387e-04\n",
            "Epoch 47/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2363e-04 - val_loss: 4.6612e-04\n",
            "Epoch 48/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3335e-04 - val_loss: 4.2962e-04\n",
            "Epoch 49/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.3201e-04 - val_loss: 4.3562e-04\n",
            "Epoch 50/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3041e-04 - val_loss: 5.0966e-04\n",
            "Epoch 51/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 9.4435e-04\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0009950100645655766.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4473e-04 - val_loss: 4.4418e-04\n",
            "Epoch 52/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.1071e-04 - val_loss: 8.6542e-04\n",
            "Epoch 53/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2430e-04 - val_loss: 4.9236e-04\n",
            "Epoch 54/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.6530e-04 - val_loss: 4.5688e-04\n",
            "Epoch 55/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.2318e-04 - val_loss: 4.6250e-04\n",
            "Epoch 56/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.2686e-04 - val_loss: 4.3647e-04\n",
            "Epoch 57/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0091e-04 - val_loss: 5.0761e-04\n",
            "Epoch 58/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3031e-04 - val_loss: 9.8448e-04\n",
            "Epoch 59/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.4207e-04 - val_loss: 4.5300e-04\n",
            "Epoch 60/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4777e-04 - val_loss: 4.2526e-04\n",
            "Epoch 61/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 9.4539e-04\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0009940150111215189.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.3354e-04 - val_loss: 4.4861e-04\n",
            "Epoch 62/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7708e-04 - val_loss: 4.7920e-04\n",
            "Epoch 63/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1448e-04 - val_loss: 4.7530e-04\n",
            "Epoch 64/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1541e-04 - val_loss: 4.4142e-04\n",
            "Epoch 65/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.4748e-04 - val_loss: 4.2980e-04\n",
            "Epoch 66/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0402e-04 - val_loss: 4.3175e-04\n",
            "Epoch 67/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.0722e-04 - val_loss: 4.7102e-04\n",
            "Epoch 68/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.0969e-04 - val_loss: 4.2362e-04\n",
            "Epoch 69/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9710e-04 - val_loss: 9.8239e-04\n",
            "Epoch 70/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9648e-04 - val_loss: 4.7825e-04\n",
            "Epoch 71/200\n",
            "281/285 [============================>.] - ETA: 0s - loss: 9.0066e-04\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0009930210043676197.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0045e-04 - val_loss: 4.3082e-04\n",
            "Epoch 72/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0837e-04 - val_loss: 6.3338e-04\n",
            "Epoch 73/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1671e-04 - val_loss: 5.4154e-04\n",
            "Epoch 74/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1453e-04 - val_loss: 4.7703e-04\n",
            "Epoch 75/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.9371e-04 - val_loss: 5.1884e-04\n",
            "Epoch 76/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8320e-04 - val_loss: 7.1645e-04\n",
            "Epoch 77/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9448e-04 - val_loss: 4.8601e-04\n",
            "Epoch 78/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0536e-04 - val_loss: 8.2641e-04\n",
            "Epoch 79/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7420e-04 - val_loss: 6.8964e-04\n",
            "Epoch 80/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9637e-04 - val_loss: 6.0215e-04\n",
            "Epoch 81/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 8.9587e-04\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0009920279280049727.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9006e-04 - val_loss: 5.2256e-04\n",
            "Epoch 82/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9136e-04 - val_loss: 5.7075e-04\n",
            "Epoch 83/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0279e-04 - val_loss: 4.4921e-04\n",
            "Epoch 84/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8467e-04 - val_loss: 4.5362e-04\n",
            "Epoch 85/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.9007e-04 - val_loss: 4.4414e-04\n",
            "Epoch 86/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8566e-04 - val_loss: 4.6630e-04\n",
            "Epoch 87/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8201e-04 - val_loss: 4.3540e-04\n",
            "Epoch 88/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1154e-04 - val_loss: 4.5040e-04\n",
            "Epoch 89/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8588e-04 - val_loss: 4.4513e-04\n",
            "Epoch 90/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.8439e-04 - val_loss: 4.9811e-04\n",
            "Epoch 91/200\n",
            "279/285 [============================>.] - ETA: 0s - loss: 9.1523e-04\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.000991035898332484.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.1234e-04 - val_loss: 4.4243e-04\n",
            "Epoch 92/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.1657e-04 - val_loss: 4.4748e-04\n",
            "Epoch 93/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0753e-04 - val_loss: 4.2903e-04\n",
            "Epoch 94/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.0279e-04 - val_loss: 4.7627e-04\n",
            "Epoch 95/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8075e-04 - val_loss: 4.5603e-04\n",
            "Epoch 96/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.1968e-04 - val_loss: 7.0155e-04\n",
            "Epoch 97/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.9135e-04 - val_loss: 4.3426e-04\n",
            "Epoch 98/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.7939e-04 - val_loss: 4.5009e-04\n",
            "Epoch 99/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.7391e-04 - val_loss: 5.3045e-04\n",
            "Epoch 100/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 9.0009e-04 - val_loss: 6.5741e-04\n",
            "Epoch 101/200\n",
            "282/285 [============================>.] - ETA: 0s - loss: 8.8513e-04\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.000990044915350154.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.8510e-04 - val_loss: 4.5384e-04\n",
            "Epoch 102/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7395e-04 - val_loss: 8.2060e-04\n",
            "Epoch 103/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.7963e-04 - val_loss: 4.4138e-04\n",
            "Epoch 104/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8830e-04 - val_loss: 5.3240e-04\n",
            "Epoch 105/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.9674e-04 - val_loss: 4.8632e-04\n",
            "Epoch 106/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.7623e-04 - val_loss: 4.6739e-04\n",
            "Epoch 107/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.0422e-04 - val_loss: 4.5062e-04\n",
            "Epoch 108/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.8954e-04 - val_loss: 5.8465e-04\n",
            "Epoch 109/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 9.1807e-04 - val_loss: 4.5590e-04\n",
            "Epoch 110/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6463e-04 - val_loss: 4.4130e-04\n",
            "Epoch 111/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 8.6288e-04\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.000989054862759076.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6288e-04 - val_loss: 5.2245e-04\n",
            "Epoch 112/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.5861e-04 - val_loss: 6.1802e-04\n",
            "Epoch 113/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6203e-04 - val_loss: 5.7364e-04\n",
            "Epoch 114/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7513e-04 - val_loss: 4.4862e-04\n",
            "Epoch 115/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4115e-04 - val_loss: 4.3391e-04\n",
            "Epoch 116/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.8153e-04 - val_loss: 6.4350e-04\n",
            "Epoch 117/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5834e-04 - val_loss: 4.5887e-04\n",
            "Epoch 118/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6296e-04 - val_loss: 4.3609e-04\n",
            "Epoch 119/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7173e-04 - val_loss: 4.5333e-04\n",
            "Epoch 120/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4936e-04 - val_loss: 5.1308e-04\n",
            "Epoch 121/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 8.6061e-04\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 0.0009880658568581567.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6059e-04 - val_loss: 6.2190e-04\n",
            "Epoch 122/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.7900e-04 - val_loss: 4.5895e-04\n",
            "Epoch 123/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.7365e-04 - val_loss: 4.4870e-04\n",
            "Epoch 124/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2740e-04 - val_loss: 5.0314e-04\n",
            "Epoch 125/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.3577e-04 - val_loss: 5.6171e-04\n",
            "Epoch 126/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5928e-04 - val_loss: 4.4407e-04\n",
            "Epoch 127/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.5363e-04 - val_loss: 5.6512e-04\n",
            "Epoch 128/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6320e-04 - val_loss: 6.0783e-04\n",
            "Epoch 129/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4433e-04 - val_loss: 4.4505e-04\n",
            "Epoch 130/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5575e-04 - val_loss: 5.2336e-04\n",
            "Epoch 131/200\n",
            "274/285 [===========================>..] - ETA: 0s - loss: 7.7301e-04\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0009870777813484892.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.3594e-04 - val_loss: 4.8937e-04\n",
            "Epoch 132/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.5449e-04 - val_loss: 4.8153e-04\n",
            "Epoch 133/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.5809e-04 - val_loss: 4.3105e-04\n",
            "Epoch 134/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.4174e-04 - val_loss: 4.4707e-04\n",
            "Epoch 135/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5520e-04 - val_loss: 6.1038e-04\n",
            "Epoch 136/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5290e-04 - val_loss: 4.8349e-04\n",
            "Epoch 137/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2658e-04 - val_loss: 4.2961e-04\n",
            "Epoch 138/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4464e-04 - val_loss: 4.9323e-04\n",
            "Epoch 139/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.6072e-04 - val_loss: 4.9126e-04\n",
            "Epoch 140/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4352e-04 - val_loss: 4.4245e-04\n",
            "Epoch 141/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 8.5589e-04\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 0.0009860907525289803.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4896e-04 - val_loss: 4.2463e-04\n",
            "Epoch 142/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4644e-04 - val_loss: 4.2569e-04\n",
            "Epoch 143/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.3298e-04 - val_loss: 3.9297e-04\n",
            "Epoch 144/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.4404e-04 - val_loss: 3.9989e-04\n",
            "Epoch 145/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2217e-04 - val_loss: 4.5520e-04\n",
            "Epoch 146/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4945e-04 - val_loss: 4.3209e-04\n",
            "Epoch 147/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2848e-04 - val_loss: 5.4956e-04\n",
            "Epoch 148/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.6057e-04 - val_loss: 4.3692e-04\n",
            "Epoch 149/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.5180e-04 - val_loss: 5.5794e-04\n",
            "Epoch 150/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2383e-04 - val_loss: 5.0060e-04\n",
            "Epoch 151/200\n",
            "273/285 [===========================>..] - ETA: 0s - loss: 8.2278e-04\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.0009851046541007236.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1900e-04 - val_loss: 4.3608e-04\n",
            "Epoch 152/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2818e-04 - val_loss: 4.9055e-04\n",
            "Epoch 153/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3815e-04 - val_loss: 5.2305e-04\n",
            "Epoch 154/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1747e-04 - val_loss: 5.5538e-04\n",
            "Epoch 155/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1123e-04 - val_loss: 4.2191e-04\n",
            "Epoch 156/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1742e-04 - val_loss: 4.6551e-04\n",
            "Epoch 157/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2417e-04 - val_loss: 5.8008e-04\n",
            "Epoch 158/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.3785e-04 - val_loss: 4.8652e-04\n",
            "Epoch 159/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2421e-04 - val_loss: 4.4890e-04\n",
            "Epoch 160/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3638e-04 - val_loss: 4.5835e-04\n",
            "Epoch 161/200\n",
            "277/285 [============================>.] - ETA: 0s - loss: 8.1683e-04\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 0.0009841196023626254.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1627e-04 - val_loss: 4.7542e-04\n",
            "Epoch 162/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2611e-04 - val_loss: 4.2439e-04\n",
            "Epoch 163/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3024e-04 - val_loss: 5.2472e-04\n",
            "Epoch 164/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1845e-04 - val_loss: 5.3219e-04\n",
            "Epoch 165/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3234e-04 - val_loss: 4.8584e-04\n",
            "Epoch 166/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1018e-04 - val_loss: 4.3346e-04\n",
            "Epoch 167/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2367e-04 - val_loss: 4.3258e-04\n",
            "Epoch 168/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2423e-04 - val_loss: 4.9459e-04\n",
            "Epoch 169/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1693e-04 - val_loss: 4.3619e-04\n",
            "Epoch 170/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1339e-04 - val_loss: 4.3894e-04\n",
            "Epoch 171/200\n",
            "275/285 [===========================>..] - ETA: 0s - loss: 8.2814e-04\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.000983135481015779.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2161e-04 - val_loss: 5.0095e-04\n",
            "Epoch 172/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.1352e-04 - val_loss: 4.6936e-04\n",
            "Epoch 173/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1318e-04 - val_loss: 5.1774e-04\n",
            "Epoch 174/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0335e-04 - val_loss: 4.0705e-04\n",
            "Epoch 175/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0986e-04 - val_loss: 4.3091e-04\n",
            "Epoch 176/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.2636e-04 - val_loss: 4.6327e-04\n",
            "Epoch 177/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2898e-04 - val_loss: 5.2755e-04\n",
            "Epoch 178/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.8533e-04 - val_loss: 4.9510e-04\n",
            "Epoch 179/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.8413e-04 - val_loss: 5.3197e-04\n",
            "Epoch 180/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0577e-04 - val_loss: 4.5197e-04\n",
            "Epoch 181/200\n",
            "283/285 [============================>.] - ETA: 0s - loss: 8.0528e-04\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 0.0009821522900601849.\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0670e-04 - val_loss: 4.3454e-04\n",
            "Epoch 182/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.1080e-04 - val_loss: 4.9978e-04\n",
            "Epoch 183/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9471e-04 - val_loss: 5.1195e-04\n",
            "Epoch 184/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.8359e-04 - val_loss: 4.5255e-04\n",
            "Epoch 185/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0813e-04 - val_loss: 6.1856e-04\n",
            "Epoch 186/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.8097e-04 - val_loss: 4.2010e-04\n",
            "Epoch 187/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9528e-04 - val_loss: 4.0903e-04\n",
            "Epoch 188/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9644e-04 - val_loss: 3.9952e-04\n",
            "Epoch 189/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9269e-04 - val_loss: 6.2708e-04\n",
            "Epoch 190/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.3701e-04 - val_loss: 4.7424e-04\n",
            "Epoch 191/200\n",
            "285/285 [==============================] - ETA: 0s - loss: 8.0504e-04\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 0.0009811701457947493.\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0504e-04 - val_loss: 4.3072e-04\n",
            "Epoch 192/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.0480e-04 - val_loss: 4.8613e-04\n",
            "Epoch 193/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 7.9991e-04 - val_loss: 4.3841e-04\n",
            "Epoch 194/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9946e-04 - val_loss: 4.2353e-04\n",
            "Epoch 195/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9857e-04 - val_loss: 4.4627e-04\n",
            "Epoch 196/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.4514e-04 - val_loss: 4.4238e-04\n",
            "Epoch 197/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.3409e-04 - val_loss: 4.3109e-04\n",
            "Epoch 198/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 7.9603e-04 - val_loss: 4.1527e-04\n",
            "Epoch 199/200\n",
            "285/285 [==============================] - 1s 5ms/step - loss: 8.0548e-04 - val_loss: 4.7796e-04\n",
            "Epoch 200/200\n",
            "285/285 [==============================] - 1s 4ms/step - loss: 8.2916e-04 - val_loss: 5.2625e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK58Imwkpoop"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeqmKjc1p3Tq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUBwtOtHqF71"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVAWnLVhqUrx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7hrCa8KqjO6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5rGjEP8qx8a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK2M7iTYrBBl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmmS4BdjrPso"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "381Hbsb7rew6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gh6AajWruix"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pesLLvkXr7Ua"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-Oz5ntssJ7h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C436Xf8ssc7U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l1eUchJsvyk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhv9wtSas8lA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUWYZZv6tOQG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFOQXvLotmtJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "tUrNdWtVhNVV",
        "outputId": "b9b1751b-0a49-499a-af1f-998fd080f366"
      },
      "source": [
        "clarke_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-23d4bfda6fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclarke_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'clarke_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXJKovCihUru"
      },
      "source": [
        "err_results"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}